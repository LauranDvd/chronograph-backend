{
 "cells": [
  {
   "cell_type": "code",
   "id": "243cbc2d30314092",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T05:24:20.069464Z",
     "start_time": "2025-05-13T05:24:20.048593Z"
    }
   },
   "source": [
    "from project_constants import DATABASE_PATH, LM_STUDIO_URL\n",
    "from project_constants import EMBEDDING_MODEL_NAME\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "from langchain.vectorstores import Chroma\n",
    "DATABASE_PATH = \"./../\"+DATABASE_PATH.split(\"/\")[1]+\"/\"+DATABASE_PATH.split(\"/\")[2]\n",
    "import torch\n",
    "embedding_model=EMBEDDING_MODEL_NAME\n"
   ],
   "outputs": [],
   "execution_count": 198
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T05:24:26.336772Z",
     "start_time": "2025-05-13T05:24:20.111603Z"
    }
   },
   "cell_type": "code",
   "source": "!deepeval set-local-model --model-name=\"meta-llama-3.1-8b-instruct\" --base-url=\"http://localhost:4500/v1/\" --api-key=\"test\"",
   "id": "7d8f59f74525fdf0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "+--------------------- Traceback (most recent call last) ---------------------+\n",
      "| D:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\deepeval\\cli\\main. |\n",
      "| py:303 in set_local_model_env                                               |\n",
      "|                                                                             |\n",
      "|   300         KEY_FILE_HANDLER.write_key(KeyValues.LOCAL_MODEL_FORMAT, form |\n",
      "|   301     KEY_FILE_HANDLER.write_key(KeyValues.USE_LOCAL_MODEL, \"YES\")      |\n",
      "|   302     KEY_FILE_HANDLER.write_key(KeyValues.USE_AZURE_OPENAI, \"NO\")      |\n",
      "| > 303     print(                                                            |\n",
      "|   304         \":raising_hands: Congratulations! You're now using a local mo |\n",
      "|   305     )                                                                 |\n",
      "|   306                                                                       |\n",
      "|                                                                             |\n",
      "| +----------------- locals ------------------+                               |\n",
      "| |    api_key = 'test'                       |                               |\n",
      "| |   base_url = 'http://localhost:4500/v1/'  |                               |\n",
      "| |     format = 'json'                       |                               |\n",
      "| | model_name = 'meta-llama-3.1-8b-instruct' |                               |\n",
      "| +-------------------------------------------+                               |\n",
      "|                                                                             |\n",
      "| D:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\rich\\__init__.py:7 |\n",
      "| 4 in print                                                                  |\n",
      "|                                                                             |\n",
      "|    71     from .console import Console                                      |\n",
      "|    72                                                                       |\n",
      "|    73     write_console = get_console() if file is None else Console(file=f |\n",
      "| >  74     return write_console.print(*objects, sep=sep, end=end)            |\n",
      "|    75                                                                       |\n",
      "|    76                                                                       |\n",
      "|    77 def print_json(                                                       |\n",
      "|                                                                             |\n",
      "| +-------------------------------- locals ---------------------------------+ |\n",
      "| |           end = '\\n'                                                    | |\n",
      "| |          file = None                                                    | |\n",
      "| |         flush = False                                                   | |\n",
      "| |       objects = (                                                       | |\n",
      "| |                     \":raising_hands: Congratulations! You're now using  | |\n",
      "| |                 a local model for all evals th\"+18,                     | |\n",
      "| |                 )                                                       | |\n",
      "| |           sep = ' '                                                     | |\n",
      "| | write_console = <console width=79 ColorSystem.WINDOWS>                  | |\n",
      "| +-------------------------------------------------------------------------+ |\n",
      "|                                                                             |\n",
      "| D:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\rich\\console.py:16 |\n",
      "| 78 in print                                                                 |\n",
      "|                                                                             |\n",
      "|   1675                 overflow = \"ignore\"                                  |\n",
      "|   1676             crop = False                                             |\n",
      "|   1677         render_hooks = self._render_hooks[:]                         |\n",
      "| > 1678         with self:                                                   |\n",
      "|   1679             renderables = self._collect_renderables(                 |\n",
      "|   1680                 objects,                                             |\n",
      "|   1681                 sep,                                                 |\n",
      "|                                                                             |\n",
      "| +-------------------------------- locals ---------------------------------+ |\n",
      "| |  buffer_extend = <built-in method extend of list object at              | |\n",
      "| |                  0x000002AE076099C0>                                    | |\n",
      "| |           crop = True                                                   | |\n",
      "| |          emoji = None                                                   | |\n",
      "| |            end = '\\n'                                                   | |\n",
      "| |         extend = <built-in method extend of list object at              | |\n",
      "| |                  0x000002AE076198C0>                                    | |\n",
      "| |         height = None                                                   | |\n",
      "| |      highlight = None                                                   | |\n",
      "| |        justify = None                                                   | |\n",
      "| |           line = [Segment('an LLM.'), Segment('\\n')]                    | |\n",
      "| |         markup = None                                                   | |\n",
      "| | new_line_start = False                                                  | |\n",
      "| |   new_segments = [                                                      | |\n",
      "| |                      Segment(                                           | |\n",
      "| |                          \"\\U0001f64c Congratulations! You're now using a local  | |\n",
      "| |                  model for all evals that require \\nan\"+5               | |\n",
      "| |                      ),                                                 | |\n",
      "| |                      Segment('\\n')                                      | |\n",
      "| |                  ]                                                      | |\n",
      "| |        no_wrap = None                                                   | |\n",
      "| |        objects = (                                                      | |\n",
      "| |                      \":raising_hands: Congratulations! You're now using | |\n",
      "| |                  a local model for all evals th\"+18,                    | |\n",
      "| |                  )                                                      | |\n",
      "| |       overflow = None                                                   | |\n",
      "| |         render = <bound method Console.render of <console width=79      | |\n",
      "| |                  ColorSystem.WINDOWS>>                                  | |\n",
      "| |   render_hooks = []                                                     | |\n",
      "| | render_options = ConsoleOptions(                                        | |\n",
      "| |                      size=ConsoleDimensions(width=79, height=25),       | |\n",
      "| |                      legacy_windows=True,                               | |\n",
      "| |                      min_width=1,                                       | |\n",
      "| |                      max_width=79,                                      | |\n",
      "| |                      is_terminal=True,                                  | |\n",
      "| |                      encoding='cp1252',                                 | |\n",
      "| |                      max_height=25,                                     | |\n",
      "| |                      justify=None,                                      | |\n",
      "| |                      overflow=None,                                     | |\n",
      "| |                      no_wrap=None,                                      | |\n",
      "| |                      highlight=None,                                    | |\n",
      "| |                      markup=None,                                       | |\n",
      "| |                      height=None                                        | |\n",
      "| |                  )                                                      | |\n",
      "| |     renderable = <text \"\\U0001f64c Congratulations! You're now using a local    | |\n",
      "| |                  model for all evals that require an LLM.\" [] ''>       | |\n",
      "| |    renderables = [                                                      | |\n",
      "| |                      <text \"\\U0001f64c Congratulations! You're now using a      | |\n",
      "| |                  local model for all evals that require an LLM.\" [] ''> | |\n",
      "| |                  ]                                                      | |\n",
      "| |           self = <console width=79 ColorSystem.WINDOWS>                 | |\n",
      "| |            sep = ' '                                                    | |\n",
      "| |      soft_wrap = False                                                  | |\n",
      "| |          style = None                                                   | |\n",
      "| |          width = None                                                   | |\n",
      "| +-------------------------------------------------------------------------+ |\n",
      "|                                                                             |\n",
      "| D:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\rich\\console.py:86 |\n",
      "| 4 in __exit__                                                               |\n",
      "|                                                                             |\n",
      "|    861                                                                      |\n",
      "|    862     def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any |\n",
      "|    863         \"\"\"Exit buffer context.\"\"\"                                   |\n",
      "| >  864         self._exit_buffer()                                          |\n",
      "|    865                                                                      |\n",
      "|    866     def begin_capture(self) -> None:                                 |\n",
      "|    867         \"\"\"Begin capturing console output. Call :meth:`end_capture`  |\n",
      "|                                                                             |\n",
      "| +---------------------- locals ----------------------+                      |\n",
      "| |  exc_type = None                                   |                      |\n",
      "| | exc_value = None                                   |                      |\n",
      "| |      self = <console width=79 ColorSystem.WINDOWS> |                      |\n",
      "| | traceback = None                                   |                      |\n",
      "| +----------------------------------------------------+                      |\n",
      "|                                                                             |\n",
      "| D:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\rich\\console.py:82 |\n",
      "| 2 in _exit_buffer                                                           |\n",
      "|                                                                             |\n",
      "|    819     def _exit_buffer(self) -> None:                                  |\n",
      "|    820         \"\"\"Leave buffer context, and render content if required.\"\"\"  |\n",
      "|    821         self._buffer_index -= 1                                      |\n",
      "| >  822         self._check_buffer()                                         |\n",
      "|    823                                                                      |\n",
      "|    824     def set_live(self, live: \"Live\") -> None:                        |\n",
      "|    825         \"\"\"Set Live instance. Used by Live context manager.          |\n",
      "|                                                                             |\n",
      "| +------------------- locals --------------------+                           |\n",
      "| | self = <console width=79 ColorSystem.WINDOWS> |                           |\n",
      "| +-----------------------------------------------+                           |\n",
      "|                                                                             |\n",
      "| D:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\rich\\console.py:20 |\n",
      "| 19 in _check_buffer                                                         |\n",
      "|                                                                             |\n",
      "|   2016             return                                                   |\n",
      "|   2017                                                                      |\n",
      "|   2018         try:                                                         |\n",
      "| > 2019             self._write_buffer()                                     |\n",
      "|   2020         except BrokenPipeError:                                      |\n",
      "|   2021             self.on_broken_pipe()                                    |\n",
      "|   2022                                                                      |\n",
      "|                                                                             |\n",
      "| +------------------- locals --------------------+                           |\n",
      "| | self = <console width=79 ColorSystem.WINDOWS> |                           |\n",
      "| +-----------------------------------------------+                           |\n",
      "|                                                                             |\n",
      "| D:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\rich\\console.py:20 |\n",
      "| 55 in _write_buffer                                                         |\n",
      "|                                                                             |\n",
      "|   2052                             if self.no_color and self._color_system: |\n",
      "|   2053                                 buffer = list(Segment.remove_color(b |\n",
      "|   2054                                                                      |\n",
      "| > 2055                             legacy_windows_render(buffer, LegacyWind |\n",
      "|   2056                         else:                                        |\n",
      "|   2057                             # Either a non-std stream on legacy Wind |\n",
      "|   2058                             text = self._render_buffer(self._buffer[ |\n",
      "|                                                                             |\n",
      "| +-------------------------------- locals ---------------------------------+ |\n",
      "| |                    buffer = [                                           | |\n",
      "| |                                 Segment(                                | |\n",
      "| |                                     \"\\U0001f64c Congratulations! You're now     | |\n",
      "| |                             using a local model for all evals that      | |\n",
      "| |                             require \"                                   | |\n",
      "| |                                 ),                                      | |\n",
      "| |                                 Segment('\\n'),                          | |\n",
      "| |                                 Segment('an LLM.'),                     | |\n",
      "| |                                 Segment('\\n')                           | |\n",
      "| |                             ]                                           | |\n",
      "| |                    fileno = 1                                           | |\n",
      "| |                      self = <console width=79 ColorSystem.WINDOWS>      | |\n",
      "| | use_legacy_windows_render = True                                        | |\n",
      "| +-------------------------------------------------------------------------+ |\n",
      "|                                                                             |\n",
      "| D:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\rich\\_windows_rend |\n",
      "| erer.py:19 in legacy_windows_render                                         |\n",
      "|                                                                             |\n",
      "|   16             if style:                                                  |\n",
      "|   17                 term.write_styled(text, style)                         |\n",
      "|   18             else:                                                      |\n",
      "| > 19                 term.write_text(text)                                  |\n",
      "|   20         else:                                                          |\n",
      "|   21             control_codes: Sequence[ControlCode] = control             |\n",
      "|   22             for control_code in control_codes:                         |\n",
      "|                                                                             |\n",
      "| +-------------------------------- locals ---------------------------------+ |\n",
      "| |  buffer = [                                                             | |\n",
      "| |               Segment(                                                  | |\n",
      "| |                   \"\\U0001f64c Congratulations! You're now using a local model   | |\n",
      "| |           for all evals that require \"                                  | |\n",
      "| |               ),                                                        | |\n",
      "| |               Segment('\\n'),                                            | |\n",
      "| |               Segment('an LLM.'),                                       | |\n",
      "| |               Segment('\\n')                                             | |\n",
      "| |           ]                                                             | |\n",
      "| | control = None                                                          | |\n",
      "| |   style = None                                                          | |\n",
      "| |    term = <rich._win32_console.LegacyWindowsTerm object at              | |\n",
      "| |           0x000002AE07614510>                                           | |\n",
      "| |    text = \"\\U0001f64c Congratulations! You're now using a local model for all   | |\n",
      "| |           evals that require \"                                          | |\n",
      "| +-------------------------------------------------------------------------+ |\n",
      "|                                                                             |\n",
      "| D:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\rich\\_win32_consol |\n",
      "| e.py:402 in write_text                                                      |\n",
      "|                                                                             |\n",
      "|   399         Args:                                                         |\n",
      "|   400             text (str): The text to write to the console              |\n",
      "|   401         \"\"\"                                                           |\n",
      "| > 402         self.write(text)                                              |\n",
      "|   403         self.flush()                                                  |\n",
      "|   404                                                                       |\n",
      "|   405     def write_styled(self, text: str, style: Style) -> None:          |\n",
      "|                                                                             |\n",
      "| +-------------------------------- locals ---------------------------------+ |\n",
      "| | self = <rich._win32_console.LegacyWindowsTerm object at                 | |\n",
      "| |        0x000002AE07614510>                                              | |\n",
      "| | text = \"\\U0001f64c Congratulations! You're now using a local model for all      | |\n",
      "| |        evals that require \"                                             | |\n",
      "| +-------------------------------------------------------------------------+ |\n",
      "|                                                                             |\n",
      "| D:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\encodings\\cp1252.py:19 in encode |\n",
      "|                                                                             |\n",
      "|    16                                                                       |\n",
      "|    17 class IncrementalEncoder(codecs.IncrementalEncoder):                  |\n",
      "|    18     def encode(self, input, final=False):                             |\n",
      "| >  19         return codecs.charmap_encode(input,self.errors,encoding_table |\n",
      "|    20                                                                       |\n",
      "|    21 class IncrementalDecoder(codecs.IncrementalDecoder):                  |\n",
      "|    22     def decode(self, input, final=False):                             |\n",
      "|                                                                             |\n",
      "| +-------------------------------- locals ---------------------------------+ |\n",
      "| | final = False                                                           | |\n",
      "| | input = \"\\U0001f64c Congratulations! You're now using a local model for all     | |\n",
      "| |         evals that require \"                                            | |\n",
      "| |  self = <encodings.cp1252.IncrementalEncoder object at                  | |\n",
      "| |         0x000002AE7C2A5AD0>                                             | |\n",
      "| +-------------------------------------------------------------------------+ |\n",
      "+-----------------------------------------------------------------------------+\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\U0001f64c' in \n",
      "position 0: character maps to <undefined>\n"
     ]
    }
   ],
   "execution_count": 199
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T05:24:26.433016Z",
     "start_time": "2025-05-13T05:24:26.427553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gc\n",
    "def clean_up_memory():\n",
    "    \"\"\"\n",
    "    Clean up memory by deleting the model and tokenizer.\n",
    "    \"\"\"\n",
    "    global llm, tokenizer\n",
    "    if 'llm' in globals():\n",
    "        del llm\n",
    "        print(\"Model deleted\")\n",
    "    if 'tokenizer' in globals():\n",
    "        del tokenizer\n",
    "        print(\"Tokenizer deleted\")\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Memory cleaned up\")\n",
    "    gc.collect()\n"
   ],
   "id": "c2dfcf588d1c07a5",
   "outputs": [],
   "execution_count": 200
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b058a5fe9afe4d6a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T05:24:26.484700Z",
     "start_time": "2025-05-13T05:24:26.471599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def optimize_model_configuration(quantization=8, model_name=\"google/gemma-2-2b-it\", force_gpu=True, lora_weights=None):\n",
    "    \"\"\"\n",
    "    Optimize the model configuration for the given quantization and model name.\n",
    "    Used when we directly load the model from the Hugging Face Hub.\n",
    "    :param quantization: precision of the model\n",
    "    :param model_name: name of the model\n",
    "    :param force_gpu: whether to force the model to load on GPU\n",
    "    :param lora_weights: path to LoRA adapter weights\n",
    "    :return: optimized model configuration\n",
    "    \"\"\"\n",
    "    print(\"Optimizing model configuration...\")\n",
    "   \n",
    "    global tokenizer, llm\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    else:\n",
    "        print(\"CUDA not available, will use CPU\")\n",
    "        force_gpu = False\n",
    "        \n",
    "    if quantization == 4:\n",
    "        print(\"Optimizing for maximum speed (4-bit quantization)\")\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            llm_int8_enable_fp32_cpu_offload=not force_gpu\n",
    "        )\n",
    "    elif quantization == 8:\n",
    "        print(\"Using balanced settings (8-bit quantization)\")\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            llm_int8_enable_fp32_cpu_offload=not force_gpu,\n",
    "            llm_int8_skip_modules=[\"lm_head\"]\n",
    "        )\n",
    "    elif quantization == 16:\n",
    "        print(\"Optimizing for quality (16-bit precision)\")\n",
    "        quantization_config = None\n",
    "    else:\n",
    "        raise ValueError(\"Invalid quantization value. Use 4, 8, or 16.\")\n",
    "    \n",
    "    print(\"Loading model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if not tokenizer.pad_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    if force_gpu and torch.cuda.is_available():\n",
    "        device_map = {\"\": 0} \n",
    "        print(\"Forcing model to load on GPU\")\n",
    "    else:\n",
    "        device_map = \"auto\"\n",
    "    \n",
    "    model_kwargs = {\n",
    "        \"device_map\": device_map,\n",
    "        \"torch_dtype\": torch.float16,\n",
    "    }\n",
    "    \n",
    "    if quantization_config:\n",
    "        model_kwargs[\"quantization_config\"] = quantization_config\n",
    "        \n",
    "    try:\n",
    "        # Load base model\n",
    "        if lora_weights:\n",
    "            from peft import PeftModel, PeftConfig\n",
    "            \n",
    "            print(f\"Loading base model with LoRA adapter from {lora_weights}\")\n",
    "            config = PeftConfig.from_pretrained(lora_weights)\n",
    "            \n",
    "            # First load the base model\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name if config.base_model_name_or_path is None else config.base_model_name_or_path,\n",
    "                **model_kwargs\n",
    "            )\n",
    "            \n",
    "            # Then load the LoRA adapter\n",
    "            llm = PeftModel.from_pretrained(base_model, lora_weights)\n",
    "            print(\"LoRA adapter loaded successfully\")\n",
    "        else:\n",
    "            # Regular model loading without LoRA\n",
    "            llm = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                **model_kwargs\n",
    "            )\n",
    "            print(\"Base model loaded successfully\")\n",
    "        \n",
    "        device_location = next(llm.parameters()).device\n",
    "        print(f\"Model loaded on: {device_location}\")\n",
    "        \n",
    "        if 'cuda' not in str(device_location) and torch.cuda.is_available():\n",
    "            print(\"Warning: Model loaded on CPU despite CUDA being available\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        if force_gpu:\n",
    "            print(\"Falling back to CPU loading\")\n",
    "            return optimize_model_configuration(quantization, model_name, force_gpu=False, lora_weights=lora_weights)\n",
    "    \n",
    "    return {\n",
    "        \"model_size\": sum(p.numel() for p in llm.parameters()) / 1e6,\n",
    "        \"model_device\": next(llm.parameters()).device,\n",
    "        \"quantization\": quantization,\n",
    "        \"lora_weights\": lora_weights\n",
    "    }"
   ],
   "id": "78cad100177c157c",
   "outputs": [],
   "execution_count": 201
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T05:24:26.523815Z",
     "start_time": "2025-05-13T05:24:26.518380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_huggingface_model(model_name=\"google/gemma-2-2b-it\", quantization=8, force_gpu=True, lora_weights=None):\n",
    "    \"\"\"\n",
    "    Load a model from Hugging Face Hub\n",
    "    :param model_name: name of the model to load\n",
    "    :param quantization: level of quantization (4, 8, or 16 bit)\n",
    "    :param force_gpu: whether to force GPU usage\n",
    "    :param lora_weights: path to LoRA adapter weights\n",
    "    :return: model configuration\n",
    "    \"\"\"\n",
    "    clean_up_memory()\n",
    "    return optimize_model_configuration(quantization, model_name, force_gpu, lora_weights)\n"
   ],
   "id": "9155802831de1e74",
   "outputs": [],
   "execution_count": 202
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T05:24:26.569876Z",
     "start_time": "2025-05-13T05:24:26.563059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_lm_studio_connection(url=LM_STUDIO_URL, api_key=\"lm-studio\"):\n",
    "    \"\"\"\n",
    "    Test the connection to LM Studio\n",
    "    :param url: URL of LM Studio API\n",
    "    :param api_key: API key for LM Studio\n",
    "    :return: connection info\n",
    "    \"\"\"\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    \n",
    "    try:\n",
    "        model = ChatOpenAI(\n",
    "            base_url=url,\n",
    "            api_key=api_key,\n",
    "            temperature=0.3,\n",
    "            model=\"gemma-3-12b-it\", \n",
    "        )\n",
    "        \n",
    "        response = model.invoke([{\"role\": \"user\", \"content\": \"Hello, are you connected?\"}])\n",
    "        print(f\"Successfully connected to LM Studio at {url}\")\n",
    "        print(f\"Test response: {response.content[:50]}...\")\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"connected\",\n",
    "            \"url\": url,\n",
    "            \"model\": model\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to LM Studio: {e}\")\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e)\n",
    "        }"
   ],
   "id": "3192b687579e5ab7",
   "outputs": [],
   "execution_count": 203
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# RAG Evaluation Functions"
   ],
   "id": "57c219ce61d7d3ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T05:24:26.620123Z",
     "start_time": "2025-05-13T05:24:26.609210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from embedding_model import EmbeddingModel\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "def test_lm_studio_model(test_questions, model_name=\"gemma-3-12b-it\"):\n",
    "    \"\"\"\n",
    "    Test the model using LM Studio with your existing architecture\n",
    "    :param test_questions: list of test questions\n",
    "    :param model_name: name of the model in LM Studio\n",
    "    :return: dict with results\n",
    "    \"\"\"\n",
    "    from project_constants import LM_STUDIO_URL\n",
    "    \n",
    "    print(f\"Testing LM Studio model: {model_name}\")\n",
    "    \n",
    "    # Initialize embedding model and database\n",
    "    embedding = EmbeddingModel()\n",
    "    vectordb = Chroma(\n",
    "        embedding_function=embedding,\n",
    "        persist_directory=DATABASE_PATH,\n",
    "    )\n",
    "    print(f\"Vector store loaded with {vectordb._collection.count()} documents\")\n",
    "    \n",
    "    # Initialize LM Studio model\n",
    "    model = ChatOpenAI(\n",
    "        base_url=LM_STUDIO_URL,\n",
    "        api_key=\"lm-studio\",\n",
    "        temperature=0.3,\n",
    "        model=model_name,\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, question in enumerate(test_questions):\n",
    "        print(f\"[{i+1}/{len(test_questions)}] Processing: {question}\")\n",
    "        \n",
    "        # Create chat history\n",
    "        chat_history = [{\"role\": \"user\", \"content\": question}]\n",
    "        \n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        embedded_query = embedding.embed_query(question)\n",
    "        retrieval_start = time.time()\n",
    "        retrieved_chunks = vectordb.similarity_search_by_vector(embedded_query)\n",
    "        retrieval_time = time.time() - retrieval_start\n",
    "        \n",
    "        # Format chunks\n",
    "        formatted_chunks = \"\"\n",
    "        for chunk in retrieved_chunks:\n",
    "            formatted_chunks += f\"- {chunk.page_content}\\n\"\n",
    "        \n",
    "        # Augment prompt\n",
    "        augmented_prompt = f\"{question}\\nHere are some chunks of information that could help you, they might be out of order. You should use them only if they are relevant to my question.\\nThe chunks are:{formatted_chunks}\"\n",
    "        chat_history[0][\"content\"] = augmented_prompt\n",
    "        \n",
    "        # Generate response\n",
    "        generation_start = time.time()\n",
    "        response = model.invoke(chat_history)\n",
    "        generation_time = time.time() - generation_start\n",
    "        \n",
    "        # Calculate total time\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        # Store result\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"response\": response.content,\n",
    "            \"source\": \"lm_studio\",\n",
    "            \"retrieved_chunks\": retrieved_chunks,\n",
    "            \"retrieval_time\": retrieval_time,\n",
    "            \"generation_time\": generation_time,\n",
    "            \"total_time\": total_time\n",
    "        }\n",
    "        \n",
    "        print(f\"Response: {result['response'][:100]}...\")\n",
    "        print(f\"Time: {total_time:.2f}s (retrieval: {retrieval_time:.2f}s, generation: {generation_time:.2f}s)\")\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    return results"
   ],
   "id": "329c2ee3987cb389",
   "outputs": [],
   "execution_count": 204
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T05:24:26.683215Z",
     "start_time": "2025-05-13T05:24:26.674749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_huggingface_model(test_questions, model_name=\"google/gemma-2-2b-it\", quantization=8, lora_weights=None):\n",
    "    \"\"\"\n",
    "    Test the model loaded directly from Hugging Face\n",
    "    :param test_questions: list of test questions\n",
    "    :param model_name: name of the model in Hugging Face\n",
    "    :param quantization: quantization level (4, 8, or 16)\n",
    "    :param lora_weights: path to LoRA adapter weights\n",
    "    :return: dict with results\n",
    "    \"\"\"\n",
    "    print(f\"Testing Hugging Face model: {model_name}\" + (f\" with LoRA adapter: {lora_weights}\" if lora_weights else \"\"))\n",
    "    \n",
    "    # Load the HuggingFace model\n",
    "    load_huggingface_model(model_name=model_name, quantization=quantization, lora_weights=lora_weights)\n",
    "    \n",
    "    global llm, tokenizer\n",
    "    \n",
    "    # Initialize embedding model and database\n",
    "    embedding = EmbeddingModel()\n",
    "    vectordb = Chroma(\n",
    "        embedding_function=embedding,\n",
    "        persist_directory=DATABASE_PATH,\n",
    "    )\n",
    "    print(f\"Vector store loaded with {vectordb._collection.count()} documents\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, question in enumerate(test_questions):\n",
    "        print(f\"[{i+1}/{len(test_questions)}] Processing: {question}\")\n",
    "        \n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Retrieval\n",
    "        retrieval_start = time.time()\n",
    "        embedded_query = embedding.embed_query(question)\n",
    "        retrieved_chunks = vectordb.similarity_search_by_vector(embedded_query)\n",
    "        retrieval_time = time.time() - retrieval_start\n",
    "        \n",
    "        # Format context\n",
    "        context = \"\"\n",
    "        for chunk in retrieved_chunks:\n",
    "            context += f\"{chunk.page_content}\\n\\n\"\n",
    "        \n",
    "        prompt = f\"\"\"### Instruction: \n",
    "You are a knowledgeable history tutor. Answer the following question accurately based on the provided historical context.\n",
    "Use ONLY the information provided in the context. If you don't know, say \"I don't have enough information to answer that.\"\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\n",
    "\"\"\"\n",
    "        \n",
    "        # Generate response\n",
    "        generation_start = time.time()\n",
    "        try:\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(llm.device)\n",
    "            outputs = llm.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=512,\n",
    "                temperature=0.3,\n",
    "                do_sample=True\n",
    "            )\n",
    "            response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            print(f\"Response: {response_text}...\")\n",
    "            \n",
    "            # Extract just the answer part if possible\n",
    "            if \"### Answer:\" in response_text:\n",
    "                response_text = response_text.split(\"### Answer:\")[1].strip()\n",
    "            \n",
    "            generation_time = time.time() - generation_start\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response: {e}\")\n",
    "            response_text = f\"Error: {str(e)}\"\n",
    "            generation_time = time.time() - generation_start\n",
    "        \n",
    "        # Calculate total time\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        # Store result\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"response\": response_text,\n",
    "            \"source\": \"huggingface_lora\" if lora_weights else \"huggingface\",\n",
    "            \"model_name\": model_name,\n",
    "            \"lora_weights\": lora_weights,\n",
    "            \"retrieved_chunks\": retrieved_chunks,\n",
    "            \"retrieval_time\": retrieval_time,\n",
    "            \"generation_time\": generation_time,\n",
    "            \"total_time\": total_time\n",
    "        }\n",
    "        \n",
    "        print(f\"Response: {result['response'][:100]}...\")\n",
    "        print(f\"Time: {total_time:.2f}s (retrieval: {retrieval_time:.2f}s, generation: {generation_time:.2f}s)\")\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    return results"
   ],
   "id": "23d961c3a88db399",
   "outputs": [],
   "execution_count": 205
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T05:24:26.747376Z",
     "start_time": "2025-05-13T05:24:26.722544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_with_deepeval(all_results, test_data,include_contextual=False):\n",
    "    \"\"\"\n",
    "    Evaluate model results using DeepEval metrics\n",
    "    :param all_results: List of results from different models\n",
    "    :param test_data: List of dictionaries with questions and expected answers\n",
    "    :return: Dictionary with evaluation results\n",
    "    \"\"\"\n",
    "    global test_case, evaluation\n",
    "    from deepeval import evaluate\n",
    "    from deepeval.test_case import LLMTestCase\n",
    "    from deepeval.metrics import AnswerRelevancyMetric, ContextualRelevancyMetric, HallucinationMetric\n",
    "    \n",
    "    print(\"Setting up DeepEval metrics...\")\n",
    "    metrics = [\n",
    "        AnswerRelevancyMetric(threshold=0.7),\n",
    "        HallucinationMetric(threshold=0.7)\n",
    "    ]\n",
    "    \n",
    "    contextual_index = None\n",
    "    if include_contextual:\n",
    "        from deepeval.metrics import ContextualRelevancyMetric\n",
    "        metrics.insert(1, ContextualRelevancyMetric(threshold=0.7))\n",
    "        contextual_index = 1\n",
    "    # Convert test_data to a lookup dictionary for easier access\n",
    "    test_questions_dict = {item[\"question\"]: item for item in test_data}\n",
    "    \n",
    "    # Group results by model source\n",
    "    results_by_source = {}\n",
    "    for result in all_results:\n",
    "        source = result.get(\"source\", \"unknown\")\n",
    "        if source not in results_by_source:\n",
    "            results_by_source[source] = []\n",
    "        results_by_source[source].append(result)\n",
    "    print(f\"Found {len(results_by_source)} model sources\")\n",
    "    \n",
    "    # Evaluate each model source\n",
    "    evaluation_results = {}\n",
    "    \n",
    "    for source, source_results in results_by_source.items():\n",
    "        print(f\"\\nEvaluating {source} with {len(source_results)} results...\")\n",
    "        source_evaluations = []\n",
    "        \n",
    "        for i, result in enumerate(source_results):\n",
    "            # Get question and expected answer\n",
    "            question = result.get(\"question\", \"\")\n",
    "            test_item = test_questions_dict.get(question, {})\n",
    "            expected_answer = test_item.get(\"expected_answer\", \"\")\n",
    "            \n",
    "            # Get model s response\n",
    "            response = result.get(\"response\", \"\")\n",
    "            print(f\"  Model response: {response[:30]}...\")\n",
    "            \n",
    "            # Get context from retrieved chunks\n",
    "            context = []\n",
    "            if \"retrieved_chunks\" in result and result[\"retrieved_chunks\"]:\n",
    "                chunks = result[\"retrieved_chunks\"]\n",
    "                if hasattr(chunks[0], 'page_content'):\n",
    "                    context = [chunk.page_content for chunk in chunks]\n",
    "            \n",
    "            print(f\"  Test case {i+1}: Q={question[:30]}...\")\n",
    "            \n",
    "            try:\n",
    "                # Create test case with proper context and expected output\n",
    "                test_case = LLMTestCase(\n",
    "                    input=question,\n",
    "                    actual_output=response,\n",
    "                    expected_output=expected_answer,\n",
    "                    retrieval_context=[context] if isinstance(context, str) else context,\n",
    "                    context=[context] if isinstance(context, str) else context,\n",
    "                )\n",
    "                \n",
    "                # Evaluate with metrics\n",
    "                evaluation = evaluate(\n",
    "                    test_cases=[test_case],\n",
    "                    metrics=metrics\n",
    "                )\n",
    "                \n",
    "                if evaluation and evaluation.test_results:\n",
    "                    metrics_data = evaluation.test_results[0].metrics_data\n",
    "                    \n",
    "                    # Extract scores for each metric\n",
    "                    result_item = {\n",
    "                        \"question_idx\": i,\n",
    "                        \"answer_relevancy\": metrics_data[0].score if len(metrics_data) > 0 else None,\n",
    "                        \"hallucination\": metrics_data[-1].score if len(metrics_data) > 0 else None,\n",
    "                    }\n",
    "                    \n",
    "                    # Only add contextual_relevancy if we included that metric\n",
    "                    if include_contextual and contextual_index is not None:\n",
    "                        result_item[\"contextual_relevancy\"] = metrics_data[contextual_index].score if len(metrics_data) > contextual_index else None\n",
    "                    else:\n",
    "                        result_item[\"contextual_relevancy\"] = None\n",
    "                    \n",
    "                    source_evaluations.append(result_item)\n",
    "                    \n",
    "                    print(f\"    Answer Relevancy: {result_item['answer_relevancy']:.2f}\")\n",
    "                    if include_contextual:\n",
    "                        print(f\"    Contextual Relevancy: {result_item['contextual_relevancy']:.2f if result_item['contextual_relevancy'] is not None else 'N/A'}\")\n",
    "                    print(f\"    Hallucination: {result_item['hallucination']:.2f}\")\n",
    "            except Exception as e:\n",
    "                # e de la json -> print the json\n",
    "                print(evaluation)\n",
    "                # print(test_case.to_json())\n",
    "                \n",
    "                print(f\"  Error evaluating case {i+1}: {str(e)}\")\n",
    "                continue \n",
    "        \n",
    "        \n",
    "         \n",
    "        if source_evaluations:\n",
    "            evaluation_results[source] = {\n",
    "                \"avg_answer_relevancy\": sum(e[\"answer_relevancy\"] for e in source_evaluations if e[\"answer_relevancy\"] is not None) / \n",
    "                                       len([e for e in source_evaluations if e[\"answer_relevancy\"] is not None]) if any(e[\"answer_relevancy\"] is not None for e in source_evaluations) else 0,\n",
    "                                       \n",
    "                \"avg_hallucination\": sum(e[\"hallucination\"] for e in source_evaluations if e[\"hallucination\"] is not None) / \n",
    "                                    len([e for e in source_evaluations if e[\"hallucination\"] is not None]) if any(e[\"hallucination\"] is not None for e in source_evaluations) else 0,\n",
    "                \n",
    "                \"details\": source_evaluations,\n",
    "                \"num_evaluated\": len(source_evaluations)\n",
    "            }\n",
    "            \n",
    "            # Only calculate contextual_relevancy if we included that metric\n",
    "            if include_contextual:\n",
    "                valid_contextual = [e[\"contextual_relevancy\"] for e in source_evaluations if e[\"contextual_relevancy\"] is not None]\n",
    "                if valid_contextual:\n",
    "                    evaluation_results[source][\"avg_contextual_relevancy\"] = sum(valid_contextual) / len(valid_contextual)\n",
    "                else:\n",
    "                    evaluation_results[source][\"avg_contextual_relevancy\"] = 0\n",
    "            else:\n",
    "                evaluation_results[source][\"avg_contextual_relevancy\"] = None\n",
    "            \n",
    "            # Print average scores\n",
    "            print(f\"\\n  {source} Average Results:\")\n",
    "            print(f\"    Answer Relevancy: {evaluation_results[source]['avg_answer_relevancy']:.2f}\")\n",
    "            if include_contextual:\n",
    "                print(f\"    Contextual Relevancy: {evaluation_results[source]['avg_contextual_relevancy']:.2f if evaluation_results[source]['avg_contextual_relevancy'] is not None else 'N/A'}\")\n",
    "            print(f\"    Hallucination: {evaluation_results[source]['avg_hallucination']:.2f}\")\n",
    "    \n",
    "    return evaluation_results"
   ],
   "id": "67ba0b4ca8fa3af9",
   "outputs": [],
   "execution_count": 206
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T05:24:26.874493Z",
     "start_time": "2025-05-13T05:24:26.871596Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "33de3985e6f1d558",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T05:24:26.905227Z",
     "start_time": "2025-05-13T05:24:26.888219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def benchmark_performance(topic, model_source=\"huggingface\", model_name=None, lora_weights=None):\n",
    "    \"\"\"\n",
    "    Benchmark the performance of the RAG system\n",
    "    :param topic: topic to query\n",
    "    :param model_source: source of the model (huggingface or lm_studio)\n",
    "    :param model_name: name of the model (if None, use default)\n",
    "    :param lora_weights: path to LoRA adapter weights (only used if model_source is huggingface)\n",
    "    :return: performance metrics\n",
    "    \"\"\"\n",
    "    if model_name is None:\n",
    "        model_name = \"google/gemma-2-2b-it\" if model_source == \"huggingface\" else \"gemma-3-12b-it\"\n",
    "    \n",
    "    # Initialize embedding model\n",
    "    embedding = EmbeddingModel()\n",
    "    vectordb = Chroma(\n",
    "        embedding_function=embedding,\n",
    "        persist_directory=DATABASE_PATH,\n",
    "    )\n",
    "    \n",
    "    metrics = {\n",
    "        \"retrieval_time\": 0,\n",
    "        \"generation_time\": 0,\n",
    "        \"total_time\": 0,\n",
    "        \"tokens_per_second\": 0\n",
    "    }\n",
    "    \n",
    "    # Measure overall time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Retrieve relevant documents\n",
    "    retrieval_start = time.time()\n",
    "    embedded_query = embedding.embed_query(topic)\n",
    "    chunks = vectordb.similarity_search_by_vector(embedded_query, k=5)\n",
    "    retrieval_time = time.time() - retrieval_start\n",
    "    metrics[\"retrieval_time\"] = retrieval_time\n",
    "    \n",
    "    if chunks:\n",
    "        # Generate a response\n",
    "        if model_source == \"huggingface\":\n",
    "            # Load model with or without LoRA weights\n",
    "            load_huggingface_model(model_name=model_name, lora_weights=lora_weights)\n",
    "            global llm, tokenizer\n",
    "            \n",
    "            context = \"\\n\\n\".join([chunk.page_content for chunk in chunks])\n",
    "            \n",
    "            prompt = f\"\"\"### Instruction: \n",
    "You are a knowledgeable history tutor. Answer the following question accurately based on the provided historical context.\n",
    "Use ONLY the information provided in the context. If you don't know, say \"I don't have enough information to answer that.\"\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Question:\n",
    "{topic}\n",
    "\n",
    "### Answer:\n",
    "\"\"\"\n",
    "            \n",
    "            generation_start = time.time()\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(llm.device)\n",
    "            outputs = llm.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=512,\n",
    "                temperature=0.3,\n",
    "                do_sample=True\n",
    "            )\n",
    "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Calculate token generation speed\n",
    "            input_tokens = inputs.input_ids.shape[1]\n",
    "            output_tokens = outputs.shape[1] - input_tokens\n",
    "            \n",
    "        else:  # lm_studio\n",
    "            model = ChatOpenAI(\n",
    "                base_url=LM_STUDIO_URL,\n",
    "                api_key=\"lm-studio\",\n",
    "                temperature=0.3,\n",
    "                model=model_name,\n",
    "            )\n",
    "            \n",
    "            # Format chunks\n",
    "            formatted_chunks = \"\"\n",
    "            for chunk in chunks:\n",
    "                formatted_chunks += f\"- {chunk.page_content}\\n\"\n",
    "            \n",
    "            # Create augmented prompt\n",
    "            augmented_prompt = f\"{topic}\\nHere are some chunks of information that could help you, they might be out of order. You should use them only if they are relevant to my question.\\nThe chunks are:{formatted_chunks}\"\n",
    "            \n",
    "            generation_start = time.time()\n",
    "            chat_history = [{\"role\": \"user\", \"content\": augmented_prompt}]\n",
    "            response = model.invoke(chat_history)\n",
    "            \n",
    "            # Estimate tokens for LM Studio (rough estimate)\n",
    "            input_tokens = len(augmented_prompt.split())\n",
    "            output_tokens = len(response.content.split())\n",
    "        \n",
    "        generation_time = time.time() - generation_start\n",
    "        metrics[\"generation_time\"] = generation_time\n",
    "        \n",
    "        # Calculate token generation metrics\n",
    "        if output_tokens > 0 and generation_time > 0:\n",
    "            metrics[\"tokens_per_second\"] = output_tokens / generation_time\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    metrics[\"total_time\"] = time.time() - start_time\n",
    "    \n",
    "    return metrics"
   ],
   "id": "847e47c9cfe4be90",
   "outputs": [],
   "execution_count": 207
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T05:24:26.986444Z",
     "start_time": "2025-05-13T05:24:26.965117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def visualize_evaluation_results(evaluation_results):\n",
    "    \"\"\"\n",
    "    Visualize the evaluation results with improved distinction between models\n",
    "    :param evaluation_results: evaluation results from evaluate_with_deepeval\n",
    "    :return: None (displays plots)\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    # Create dataframe for visualization\n",
    "    data = []\n",
    "    for source, results in evaluation_results.items():\n",
    "        data.append({\n",
    "            \"Model\": source,\n",
    "            \"Answer Relevancy\": results.get(\"avg_answer_relevancy\", 0),\n",
    "            \"Contextual Relevancy\": results.get(\"avg_contextual_relevancy\", 0) if results.get(\"avg_contextual_relevancy\") is not None else 0,\n",
    "            \"Hallucination\": results.get(\"avg_hallucination\", 0)\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Generate distinct colors for each model\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(df)))\n",
    "    \n",
    "    # Plot with multiple subplots for better visibility\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "    \n",
    "    # Bar chart for average scores\n",
    "    ax1 = axes[0]\n",
    "    \n",
    "    # Use different pattern styles for each metric\n",
    "    patterns = ['/', '\\\\', 'o']\n",
    "    \n",
    "    # Plot each metric with its own color scheme\n",
    "    metrics = [\"Answer Relevancy\", \"Contextual Relevancy\", \"Hallucination\"]\n",
    "    \n",
    "    # Compute bar positions\n",
    "    models = df[\"Model\"].tolist()\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.25\n",
    "    \n",
    "    # Plot each metric separately with distinct colors\n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax1.bar(x + (i - 1) * width, df[metric], width, label=metric, \n",
    "                color=plt.cm.tab10(i), hatch=patterns[i % len(patterns)])\n",
    "        \n",
    "        # Add value labels on the bars\n",
    "        for j, v in enumerate(df[metric]):\n",
    "            ax1.text(x[j] + (i - 1) * width, v + 0.02, f'{v:.2f}', \n",
    "                    ha='center', va='bottom', fontsize=8, rotation=0)\n",
    "    \n",
    "    ax1.set_title(\"Model Performance by Metric\")\n",
    "    ax1.set_ylabel(\"Score\")\n",
    "    ax1.set_ylim(0, 1.1)  # Give some space for the labels\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(models, rotation=45, ha='right')\n",
    "    ax1.legend(loc='upper right')\n",
    "    ax1.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "    \n",
    "    # Radar chart for model comparison\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    # Create angles for each metric\n",
    "    N = len(metrics)\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]  # Close the loop\n",
    "    \n",
    "    # Plot in polar coordinates\n",
    "    ax2 = plt.subplot(122, polar=True)\n",
    "    \n",
    "    # Add lines for each model with distinct colors\n",
    "    for i, row in df.iterrows():\n",
    "        values = [row[metric] for metric in metrics]\n",
    "        values += values[:1]  # Close the loop\n",
    "        \n",
    "        ax2.plot(angles, values, linewidth=2, linestyle='solid', marker='o', \n",
    "                 label=row[\"Model\"], color=colors[i])\n",
    "        ax2.fill(angles, values, alpha=0.2, color=colors[i])\n",
    "    \n",
    "    # Set labels on the axes\n",
    "    ax2.set_xticks(angles[:-1])\n",
    "    ax2.set_xticklabels(metrics)\n",
    "    \n",
    "    # Add legend with unique marker for each model\n",
    "    ax2.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    \n",
    "    ax2.set_title(\"Model Comparison Radar Chart\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_evaluation_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary table\n",
    "    print(\"\\nEvaluation Results Summary:\")\n",
    "    print(df.to_string(index=False))"
   ],
   "id": "5fc6548e885572ea",
   "outputs": [],
   "execution_count": 208
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T05:24:27.091670Z",
     "start_time": "2025-05-13T05:24:27.045272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_test_questions_json():\n",
    "    \"\"\"Create a JSON file with test questions and expected answers in historical summary style.\"\"\"\n",
    "    import json\n",
    "    import os\n",
    "    \n",
    "    test_questions = [\n",
    "        {\n",
    "            \"question\": \"What were the main causes of World War I?\",\n",
    "            \"expected_answer\": \"\"\"The main causes of World War I can be traced to several interconnected factors. Nationalism played a significant role, particularly in the Balkans where ethnic tensions were high. This was closely tied to imperialism, as European powers competed for colonies and resources globally, creating rivalries and tensions. The complex system of alliances that developed in Europe meant that when conflicts arose, they quickly escalated as countries were obligated to support their allies. Germany's alliance with Austria-Hungary and Russia's commitment to Serbia exemplified this problem.\n",
    "\n",
    "Militarism was another crucial factor, with European powers engaging in a significant arms race in the years preceding the war. The rapid build-up of armies and navies, particularly between Britain and Germany, created an atmosphere of mutual suspicion and fear.\n",
    "\n",
    "The immediate trigger was the assassination of Archduke Franz Ferdinand of Austria-Hungary in Sarajevo on June 28, 1914, by a Serbian nationalist. This event set off a chain reaction through the alliance systems. Austria-Hungary declared war on Serbia, Russia mobilized to defend Serbia, Germany honored its alliance with Austria-Hungary, and soon most of Europe was drawn into the conflict. What might have remained a localized conflict became a global war due to these underlying tensions and entanglements.\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"How did the Industrial Revolution change society?\",\n",
    "            \"expected_answer\": \"\"\"The Industrial Revolution fundamentally transformed society in ways that continue to shape our world today. Beginning in Britain in the late 18th century and spreading across Europe and North America, this period saw a shift from agrarian, handicraft economies to ones dominated by machine manufacturing.\n",
    "\n",
    "Society experienced profound urbanization as people migrated from rural areas to cities seeking factory work. This led to the rapid growth of industrial centers and new social challenges. Working conditions in early factories were often harsh, with long hours, dangerous conditions, and child labor being common. These conditions eventually sparked labor movements and calls for reform.\n",
    "\n",
    "The revolution created new social classes. The industrial middle class of factory owners and businessmen gained economic power, while a new working class formed from the factory workforce. Traditional social hierarchies based on land ownership were disrupted as wealth from manufacturing created new paths to social status.\n",
    "\n",
    "Daily life changed dramatically with new technologies. Steam power, mechanized textile production, improved iron production, and eventually electricity transformed both manufacturing and domestic life. Transportation revolutions with railways and steamships connected previously isolated areas and facilitated trade.\n",
    "\n",
    "Family structures evolved as work moved outside the home. Women's roles changed, with many working-class women employed in factories while middle-class women often became more confined to domestic spheres. Education expanded to meet the needs of an industrializing society, with basic literacy becoming more important.\n",
    "\n",
    "The environmental impact was significant, with pollution, resource depletion, and urban crowding creating new health challenges. Overall, the Industrial Revolution marked a turning point in human history, accelerating economic growth while fundamentally altering social structures and daily life.\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What was the significance of the French Revolution?\",\n",
    "            \"expected_answer\": \"\"\"The French Revolution, which began in 1789, stands as one of history's most influential political events. Its significance stems from how it radically transformed not just France, but political thinking worldwide.\n",
    "\n",
    "The Revolution overthrew the absolute monarchy that had ruled France for centuries, establishing the principle that government authority comes from the people rather than divine right. The Declaration of the Rights of Man and of the Citizen articulated revolutionary principles of universal rights, liberty, and equality that continue to inspire democratic movements globally.\n",
    "\n",
    "Socially, the Revolution challenged the rigid class system of the Old Regime. The abolition of feudal privileges in August 1789 marked a decisive break with medieval social structures, promoting the ideal of a society based on merit rather than birth. The revolutionary slogan \"Liberty, Equality, Fraternity\" encapsulated these aspirations.\n",
    "\n",
    "The Revolution's relationship with religion was complex and consequential. The Catholic Church's privileged position was dismantled, church lands were nationalized, and efforts were made to subordinate religion to the state. This created new understandings of secularism in public life that remain influential.\n",
    "\n",
    "Politically, the Revolution introduced new concepts and vocabulary. Terms like \"left wing\" and \"right wing\" originated from the seating arrangements in the National Assembly. Revolutionary France also pioneered modern nationalism, transforming subjects into citizens with shared identity and direct relationship to the nation.\n",
    "\n",
    "The Revolution spread beyond France through both conquest and inspiration. Napoleon's subsequent campaigns spread revolutionary principles across Europe, while colonial territories and other nations were inspired to seek their own revolutionary changes.\n",
    "\n",
    "Despite the Terror and the eventual rise of Napoleon, the Revolution's ideals of liberty, equality, popular sovereignty, and nationalism profoundly shaped modern political thought and continue to resonate in contemporary society.\"\"\"\n",
    "        },\n",
    "\n",
    "        {\n",
    "            \"question\": \"How did the Cold War affect international relations?\",\n",
    "            \"expected_answer\": \"\"\"The Cold War fundamentally reshaped international relations from 1947 until 1991, creating patterns and institutions that continue to influence global politics today. At its core, this conflict between the United States and Soviet Union transformed a previously multipolar world into a bipolar one, with most nations aligning with one of the two superpowers.\n",
    "\n",
    "This bipolar structure created a unique form of international stability through the concept of mutually assured destruction. As both superpowers developed extensive nuclear arsenals, direct conflict became unthinkable, leading to what historians call \"the long peace\" among major powers. However, this nuclear standoff also generated tremendous anxiety and required new diplomatic approaches to manage the risk of catastrophic war.\n",
    "\n",
    "While direct confrontation was avoided, the Cold War spawned numerous proxy conflicts across Asia, Africa, and Latin America. Civil wars and regional conflicts in Korea, Vietnam, Angola, and elsewhere became battlegrounds for superpower competition, with devastating consequences for local populations. These conflicts were often intensified and prolonged by superpower involvement.\n",
    "\n",
    "The period saw the emergence of formal alliance systems that institutionalized the division. NATO (1949) unified Western nations under American leadership, while the Warsaw Pact (1955) formalized Soviet control over Eastern Europe. These military alliances changed how nations conceptualized security, emphasizing collective defense and integrated military planning.\n",
    "\n",
    "Decolonization occurred largely during the Cold War, with newly independent nations pressured to align with one bloc or another. The Non-Aligned Movement emerged as an attempt to maintain independence from superpower domination, though these countries often found themselves courted or coerced by both sides.\n",
    "\n",
    "International organizations like the United Nations became forums for Cold War rivalries, with the Security Council often paralyzed by superpower vetoes. Yet these organizations also provided essential venues for dialogue, conflict management, and development assistance.\n",
    "\n",
    "The rivalry extended to economic systems, with capitalism and communism presented as competing models for development. This ideological dimension influenced domestic policies worldwide as countries adopted aspects of either model, often with significant foreign assistance or pressure.\n",
    "\n",
    "When the Soviet Union collapsed in 1991, the bilateral framework that had organized international relations for nearly half a century suddenly vanished, creating both new opportunities for cooperation and new sources of instability that continue to shape our world today.\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"How did Romania transition from communism to democracy in 1989?\",\n",
    "            \"expected_answer\": \"\"\"Romania's transition from communism to democracy in 1989 stands out as one of Eastern Europe's most violent and complex revolutionary moments. Unlike the relatively peaceful transitions in neighboring countries, Romania's revolution involved significant bloodshed and dramatic public events that continue to shape its national memory.\n",
    "\n",
    "Under Nicolae Ceaușescu, Romania experienced one of the harshest communist regimes in the Eastern Bloc. By the late 1980s, Romanians faced severe food shortages, energy rationing, and brutal repression from the Securitate secret police. While other Eastern European nations were already embracing reforms through Gorbachev's policies of glasnost and perestroika, Ceaușescu steadfastly maintained his totalitarian grip.\n",
    "\n",
    "The revolution began in mid-December 1989 in Timișoara, when authorities attempted to evict Hungarian Reformed Church pastor László Tőkés, who had spoken against the regime. Local citizens gathered to protect him, triggering demonstrations that quickly spread. Security forces responded with violence, killing numerous protesters and creating martyrs for the revolutionary cause.\n",
    "\n",
    "The decisive turning point came on December 21st, when Ceaușescu organized a mass rally in Bucharest to demonstrate his control. In an unprecedented moment broadcast live on television, the crowd began booing and chanting anti-government slogans. Ceaușescu's visible shock and confusion—captured on camera—shattered the aura of invincibility surrounding his regime. Within hours, full-scale revolution engulfed Bucharest.\n",
    "\n",
    "As violence escalated, key military leaders defected to the revolutionary side, dramatically shifting the power balance. Ceaușescu and his wife Elena attempted to flee but were captured, subjected to a hasty trial, and executed on Christmas Day, with footage of their bodies shown on national television.\n",
    "\n",
    "The National Salvation Front (NSF), led by Ion Iliescu, a former Communist Party official, quickly filled the power vacuum. While claiming revolutionary legitimacy, the NSF's composition—largely former communist officials—raised questions about the authenticity of Romania's break with its communist past. This \"stolen revolution\" narrative persists in Romanian political discourse.\n",
    "\n",
    "Romania's post-revolutionary path proved challenging. Unlike some former Eastern Bloc countries that implemented rapid economic reforms, Romania experienced a more gradual, often painful transition. Economic hardship, corruption, and questions about justice for revolution-era crimes complicated its democratic development. Furthermore, the exact events of December 1989—including who was responsible for much of the violence—remain contested, with some historians suggesting elements of both genuine popular revolution and internal coup.\n",
    "\n",
    "Nevertheless, Romania eventually established democratic institutions, joined NATO in 2004 and the European Union in 2007, marking significant milestones in its post-communist journey. Despite ongoing challenges with corruption and political instability, Romania's dramatic 1989 revolution remains a defining moment in its modern history—the violent end to four decades of communist rule and the beginning of its democratic transition.\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What were the key events of the Spanish Civil War (1936-1939)?\",\n",
    "            \"expected_answer\": \"\"\"The Spanish Civil War (1936-1939) represents one of Europe's most devastating domestic conflicts, serving as both a national tragedy and an international ideological battleground that foreshadowed World War II. This complex struggle set Republican forces loyal to the democratically elected government against Nationalist rebels led by General Francisco Franco.\n",
    "\n",
    "The war emerged from Spain's polarized political landscape following the 1931 establishment of the Second Spanish Republic, which initiated ambitious reforms threatening traditional power centers. Land redistribution alienated large landowners, anticlericalism antagonized the Catholic Church, and military reforms unsettled army officers, while labor reforms and regional autonomy statutes further divided society. By 1936, Spain was essentially split between progressive, urban, secular forces and conservative, rural, Catholic traditionalists.\n",
    "\n",
    "The conflict formally began in July 1936 when military garrisons in Spanish Morocco, led by General Franco, launched a pronunciamiento (military rebellion) against the Republican government. The rebels expected quick victory, but Spain split geographically and ideologically – Nationalists controlled Spain's northwest and south, while Republicans held Madrid, Barcelona, Valencia, and industrial regions. This division ensured a prolonged, brutal conflict.\n",
    "\n",
    "The war quickly internationalized. Nazi Germany and Fascist Italy provided crucial military support to Franco, including the infamous bombing of Guernica in 1937 that inspired Picasso's masterpiece. The Soviet Union supported the Republic with arms and advisors, while approximately 40,000 foreign volunteers from 52 countries formed the International Brigades to defend the Republic. Western democracies, fearing wider conflict, adopted a policy of non-intervention that effectively disadvantaged the Republican side.\n",
    "\n",
    "The conflict became characterized by atrocities on both sides. Nationalists conducted systematic purges in territories they captured, executing left-wing political opponents, intellectuals, and labor organizers. Republicans committed their own violence, particularly early in the war, with attacks against clergy and landowners. The war saw the tactical development of modern warfare, with the urban siege of Madrid, the introduction of coordinated air and ground attacks, and the devastating targeting of civilian populations.\n",
    "\n",
    "By early 1939, Franco's forces had secured most of Spain. Barcelona fell in January, Madrid surrendered in March, and the war officially ended on April 1, 1939. The human cost was staggering – approximately 500,000 deaths from combat, execution, disease, and malnutrition, with another half-million refugees fleeing to France and elsewhere.\n",
    "\n",
    "Franco established a dictatorship that lasted until his death in 1975. His regime initially aligned with the Axis powers but maintained official neutrality during World War II. The regime was characterized by political repression, censorship, economic autarky, and Catholic traditionalism. Only after Franco's death did Spain successfully transition to democracy.\n",
    "\n",
    "The Spanish Civil War's significance extends beyond Spain's borders. Internationally, it represented the battleground between fascism and democracy at a time when Western powers were pursuing appeasement. For intellectuals and artists worldwide, it became the defining moral cause of the era, inspiring works from Hemingway, Orwell, and Picasso. Most significantly, the conflict served as a military testing ground for World War II tactics and technologies, making it, in many ways, the first battle of the larger war that would soon engulf Europe.\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"How did the unification of Germany in the 19th century reshape European politics?\",\n",
    "            \"expected_answer\": \"\"\"The unification of Germany in the 19th century fundamentally transformed European politics, reconfiguring power dynamics that had persisted since the Peace of Westphalia in 1648. This process, culminating in 1871, created a powerful new nation-state in Central Europe that dramatically altered the continental balance of power and set the stage for intense rivalries that would eventually contribute to World War I.\n",
    "\n",
    "Prior to unification, \"Germany\" existed as a cultural and linguistic concept encompassing dozens of independent states within the loose Germanic Confederation, with Austria and Prussia as the dominant powers. This fragmentation had long served the interests of neighboring countries, particularly France, which benefited from Central European disunity. The unification process challenged this established order through three critical phases, orchestrated largely by Prussian Minister President Otto von Bismarck's realpolitik approach—a pragmatic pursuit of power politics over ideological considerations.\n",
    "\n",
    "The first decisive step came with the Danish War of 1864, where Prussia and Austria cooperatively seized the duchies of Schleswig and Holstein. This partnership proved short-lived, as Bismarck engineered the Austro-Prussian War of 1866, resulting in Prussia's swift victory and Austria's exclusion from German affairs. The North German Confederation formed under Prussian leadership, while Prussia annexed several smaller states, significantly expanding its territory.\n",
    "\n",
    "Unification culminated during the Franco-Prussian War of 1870-71. Bismarck's edited version of the Ems Telegram provoked France into declaring war, enabling him to portray Prussia as defending German interests against foreign aggression. The overwhelming Prussian victory, including the humiliating capture of Emperor Napoleon III, created a wave of pan-German nationalism. On January 18, 1871, in the Hall of Mirrors at Versailles—the symbolic heart of French power—the German Empire was proclaimed with Prussia's King Wilhelm I as Kaiser.\n",
    "\n",
    "The immediate geopolitical consequence was the emergence of a dominant new power in Europe. The German Empire possessed the continent's most powerful army, a rapidly industrializing economy, and Europe's largest population after Russia. This new entity fundamentally altered the European balance of power, ending the multipolarity that had characterized international relations and creating a system increasingly defined by alliance blocs.\n",
    "\n",
    "For France, German unification represented a catastrophic strategic defeat. Beyond losing the provinces of Alsace and Lorraine, France faced a powerful, unified neighbor where previously a fragmented collection of states had provided a strategic buffer. The French desire for revanche (revenge) became a constant factor in European diplomacy.\n",
    "\n",
    "German unification also reshaped Central and Eastern European politics. Austria-Hungary, excluded from German affairs, redirected its ambitions toward the Balkans, creating tensions with Russia that would contribute to the outbreak of World War I. Meanwhile, the demonstration of German strength encouraged Italy to complete its own unification by seizing Rome.\n",
    "\n",
    "The domestic political configuration of unified Germany had profound implications for European stability. Although possessing democratic elements through the Reichstag, real power resided with the Kaiser and military leaders. This structure, combined with rapid industrialization and militarism, created a state capable of channeling national resources toward strategic objectives in ways other European powers found difficult to match.\n",
    "\n",
    "Ultimately, German unification shattered the balance of power system that had largely maintained European stability since the Napoleonic Wars. The resulting security dilemmas, arms races, and alliance structures created a more rigid international system that proved unable to contain the crisis of 1914, linking the 1871 unification directly to the cataclysm of World War I and the subsequent reshaping of Europe in the 20th century.\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What was the significance of the Hungarian Revolution of 1956?\",\n",
    "            \"expected_answer\": \"\"\"The Hungarian Revolution of 1956 stands as one of the most significant challenges to Soviet authority in Eastern Europe during the Cold War, becoming a defining moment in Hungary's national identity and revealing the limits of de-Stalinization within the Soviet bloc. Though brutally crushed after just twelve days, this spontaneous uprising against communist rule profoundly influenced Cold War dynamics, Western perceptions of Soviet communism, and ultimately the path toward Eastern European independence.\n",
    "\n",
    "The revolution emerged from Hungary's complex post-World War II experience. Soviet forces installed a communist government that implemented harsh Stalinist policies under Mátyás Rákosi, including collectivization, religious persecution, and political purges. After Stalin's death in 1953 and Khrushchev's \"Secret Speech\" denouncing Stalinist excesses in February 1956, reformist currents began flowing through Eastern Europe. In neighboring Poland, successful protests had just forced political concessions, inspiring Hungarian hopes.\n",
    "\n",
    "On October 23, 1956, a student demonstration in Budapest calling for democratic reforms grew into mass protests when Hungarian State Security (ÁVH) officers fired on the crowd. The situation escalated rapidly, with citizens toppling the massive Stalin statue in Budapest's City Park—producing iconic images of revolution. Initially, Soviet forces stationed in Hungary attempted military intervention but were overwhelmed by fierce civilian resistance using improvised weapons and tactics.\n",
    "\n",
    "By October 28, Soviet forces temporarily withdrew from Budapest, and a new government under reformist communist Imre Nagy took power. Nagy's government instituted sweeping changes—announcing Hungary's withdrawal from the Warsaw Pact, declaring neutrality, introducing democratic reforms, and releasing political prisoners. Cardinal József Mindszenty, a symbol of religious resistance imprisoned since 1949, was freed and quickly became an emblematic figure of the revolution.\n",
    "\n",
    "The revolution's peak saw extraordinary expressions of freedom—independent newspapers flourished, non-communist political parties reemerged, and workers' councils exercised genuine democratic control in factories. Revolutionary committees effectively governed at local levels, demonstrating Hungarians' remarkable capacity for self-organization amid crisis. This period, though brief, represented the authentic democratic will of the Hungarian people.\n",
    "\n",
    "The Soviet response, when it came, was overwhelming. On November 4, over 1,000 Soviet tanks entered Budapest, engaging in ferocious urban combat against lightly armed civilians and military defectors. Despite valiant resistance and desperate appeals for Western assistance, the revolution was crushed. The fighting killed approximately 2,500 Hungarians and 700 Soviet troops, with thousands more wounded.\n",
    "\n",
    "In the aftermath, reprisals were severe. The Soviets installed János Kádár as Hungary's new leader, who presided over mass arrests, secret trials, and executions—including Nagy himself in 1958, despite promises of safe passage. Approximately 200,000 Hungarians fled as refugees, creating a substantial diaspora community that kept revolutionary memory alive internationally. Inside Hungary, the revolution entered a decades-long period as an unmentionable topic, referred to euphemistically as \"the unfortunate events.\"\n",
    "\n",
    "The international impact was profound. The uprising exposed deep fissures in Western communist parties, with many members resigning in protest against Soviet actions. For the United States and its allies, despite rhetoric about \"rolling back\" communism, the revolution painfully demonstrated the limits of Western willingness to directly challenge Soviet control in its sphere of influence. The timing—coinciding with the Suez Crisis—further complicated the international response, dividing Western attention and providing the Soviets with a simultaneous distraction.\n",
    "\n",
    "The Hungarian Revolution's significance extended beyond its immediate failure. Kádár's subsequent \"Goulash Communism\" offered Hungarians greater economic freedom and reduced political repression in exchange for accepting Soviet dominance in foreign affairs—a pragmatic compromise emerging directly from 1956's lessons. More broadly, the revolution revealed that Eastern bloc stability rested on coercion rather than popular consent, a fundamental weakness that would ultimately contribute to communism's collapse in 1989, when Hungary led the way in opening its borders and dismantling the Iron Curtain.\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"How did Bulgaria's experience under Ottoman rule shape its national identity?\",\n",
    "            \"expected_answer\": \"\"\"Bulgaria's nearly five-century experience under Ottoman rule (1396-1878) profoundly shaped its national identity, creating historical narratives, cultural patterns, and geopolitical orientations that continue to influence modern Bulgarian society. This lengthy period under Islamic imperial governance both preserved and transformed Bulgarian identity, producing complex legacies that would later fuel national revival and continue to resonate in contemporary politics.\n",
    "\n",
    "The Ottoman conquest dramatically altered Bulgaria's medieval trajectory. The Second Bulgarian Empire, centered at Tarnovo, fell to Ottoman forces in 1396, ending an independent state that had been a significant regional power. Bulgarian elites faced systematic disempowerment—nobility was eliminated, the autocephalous Bulgarian Orthodox Church lost its independence to the Greek-dominated Constantinople Patriarchate, and local governance structures were replaced by Ottoman administrative systems.\n",
    "\n",
    "Religious identity became the primary marker of difference under the Ottoman millet system, which organized subjects by faith rather than ethnicity. Bulgarians belonged to the Orthodox Christian millet, administered by Greek clergy who often suppressed Bulgarian language in liturgy and education. This created a dual subordination for Bulgarians—politically under Ottoman authorities and culturally under Greek ecclesiastical control—that would later influence Bulgarian nationalism's development against both Turkish and Greek influences.\n",
    "\n",
    "The Ottoman period saw significant demographic changes in Bulgarian lands. Some regions experienced Turkish colonization, particularly in the east and along major transport routes. Urban centers developed distinct Muslim quarters, with mosques and other Islamic architecture permanently altering the built environment. Periodic conversion to Islam occurred, both forced and voluntary, creating communities like the Pomaks (Bulgarian-speaking Muslims) whose complex identities continue to challenge simplistic national narratives today.\n",
    "\n",
    "Economic exploitation characterized much of Ottoman governance in Bulgarian territories. Heavy taxation, particularly the devshirme (child levy) that conscripted Christian boys into the Janissary corps, created enduring historical traumas. Agricultural production shifted toward Ottoman imperial needs, while Bulgarian peasants increasingly retreated to mountain villages to avoid excessive burdens. However, by the 18th century, some Bulgarians found economic opportunities within Ottoman trade networks, creating a merchant class that would later fund national revival efforts.\n",
    "\n",
    "The 18th and early 19th centuries saw Ottoman power decline while Bulgarian cultural consciousness strengthened. This National Revival period (Възраждане/Vazrazhdane) was characterized by the spread of secular education in Bulgarian, the struggle for an independent Bulgarian church, and the development of a distinctive national literature. Figures like Paisii Hilendarski, whose \"Slavonic-Bulgarian History\" (1762) emphasized glorious medieval Bulgarian achievements, helped construct a national narrative centered on Ottoman oppression and the need for liberation.\n",
    "\n",
    "Armed resistance periodically challenged Ottoman rule, from the hajduk tradition of mountain outlaws to major uprisings like the April Uprising of 1876. Though brutally suppressed—with massacres that horrified international opinion—these rebellions created national martyrs and eventually precipitated Russian intervention, leading to Bulgaria's liberation in 1878 following the Russo-Turkish War.\n",
    "\n",
    "Post-liberation Bulgaria constructed national identity largely in opposition to Ottoman legacies. Historical narratives emphasized a \"Turkish yoke\" of oppression and resistance, while urban renewal often removed Ottoman architectural elements. The new state pursued policies of cultural homogenization, encouraging Turkish and Muslim emigration while assimilating remaining minority populations. However, Ottoman administrative practices, legal traditions, and cultural influences persisted, creating tensions between nationalist ideology and lived reality.\n",
    "\n",
    "Today, Bulgaria's Ottoman past remains contentious in national discourse. Historical interpretations range from viewing the period as one of unmitigated oppression to more nuanced perspectives recognizing cultural synthesis and pragmatic coexistence. Ottoman architectural heritage faces challenges of preservation amid nationalist sentiments, while Turkish and Muslim minorities continue navigating complex identity politics. Bulgaria's geopolitical orientation—between East and West—similarly reflects the enduring impact of its Ottoman experience, as the nation continues defining itself in relation to this formative historical period that simultaneously represents both cultural trauma and inescapable heritage.\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What role did Spain play in the European colonization of the Americas?\",\n",
    "            \"expected_answer\": \"\"\"Spain played a pivotal, pioneering role in the European colonization of the Americas, establishing the first transatlantic empire and creating colonial patterns that would influence subsequent European imperial projects. From Columbus's arrival in 1492 until independence movements in the early 19th century, Spanish colonization fundamentally transformed the Western Hemisphere's demographic, cultural, religious, and political landscapes while simultaneously reshaping Spain itself and the global economy.\n",
    "\n",
    "The Spanish colonial enterprise began with Christopher Columbus's voyages, sponsored by the Catholic Monarchs Ferdinand and Isabella shortly after Spain's unification and the completion of the Reconquista. Columbus's accidental encounter with the Caribbean initiated explosive Spanish expansion—within three decades, the Aztec and Inca empires had fallen to small contingents of conquistadors led by Hernán Cortés and Francisco Pizarro, respectively. By the mid-16th century, Spain controlled territories from modern-day California and Florida to Chile and Argentina, establishing the first empire where \"the sun never set.\"\n",
    "\n",
    "The early conquest phase was characterized by the encomienda system, which granted conquistadors authority over indigenous populations, ostensibly to Christianize them while extracting labor and tribute. This exploitative system, though eventually reformed under pressure from critics like Bartolomé de las Casas, set patterns of economic extraction and racial hierarchy that would define Spanish colonialism. The catastrophic demographic collapse of indigenous populations—primarily through disease but also through violence and exploitation—represents the conquest's most profound consequence, with population declines exceeding 90% in many regions.\n",
    "\n",
    "Spanish colonialism's institutional framework developed distinct characteristics. The viceroyalty system initially established administrative centers in Mexico and Peru, later expanded to New Granada and Río de la Plata. The Council of the Indies in Spain maintained centralized control, while the Casa de Contratación regulated transatlantic commerce. This elaborate bureaucratic structure represented the most sophisticated colonial administration of its era, though in practice, the principle of \"obedezco pero no cumplo\" (I obey but do not comply) often allowed local authorities to adapt or ignore metropolitan directives.\n",
    "\n",
    "The extraction of wealth—particularly silver from massive mining operations at Potosí (Bolivia) and Zacatecas (Mexico)—fundamentally altered global economics. Spain channeled unprecedented mineral wealth into European markets, fueling price revolutions, financing Habsburg imperial ambitions, and increasingly integrating global trade networks. Ironically, much of this wealth ultimately flowed to Spain's creditors and trade partners rather than developing domestic industries, contributing to Spain's long-term economic challenges despite its imperial dominance.\n",
    "\n",
    "Religious conversion stood as a primary justification for Spanish colonialism. The Catholic Church established extensive mission systems, built monumental cathedrals, and created educational institutions throughout the colonies. Religious orders—particularly Franciscans, Dominicans, and later Jesuits—played crucial roles in colonial society, sometimes advocating for indigenous rights while simultaneously dismantling native belief systems. The resulting religious landscape blended Catholic orthodoxy with indigenous spiritual elements, creating distinctive syncretic practices that persist today.\n",
    "\n",
    "Spanish colonialism produced complex new social structures. The sistema de castas categorized people according to racial ancestry, with peninsulares (Spanish-born) and criollos (American-born Spaniards) at the apex. Mestizos (mixed Spanish-indigenous), mulatos (mixed Spanish-African), indigenous peoples, and enslaved and free Africans occupied intermediate and lower positions, though considerable social fluidity existed in practice. These racial categorizations, while less rigid than later colonial systems, established frameworks for social stratification that long outlasted Spanish rule.\n",
    "\n",
    "Urban development characterized Spanish colonialism, with hundreds of cities established in regular grid patterns reflecting Renaissance ideals. These cities served as centers of Spanish political control, economic activity, and cultural influence, with central plazas typically featuring government buildings and cathedrals symbolizing the alliance of crown and church. This urban emphasis contrasted with some later European colonial models and left an enduring imprint on Latin American settlement patterns.\n",
    "\n",
    "By the late 18th century, Bourbon reforms attempted to modernize and rationalize colonial administration while extracting greater revenue, creating tensions that contributed to independence movements. Inspired by Enlightenment principles and the examples of American and French revolutions, Latin American independence movements led by figures like Simón Bolívar and José de San Martín ended most Spanish control by the 1820s, though Cuba and Puerto Rico remained Spanish until 1898.\n",
    "\n",
    "Spain's colonial legacy remains profoundly embedded in Latin America's languages, legal systems, religious practices, architectural styles, and social structures. The complex heritage of conquest, cultural exchange, and resistance continues shaping identities and politics throughout the region, while debates about historical responsibility for colonial violence and exploitation remain relevant to contemporary relations between Spain and its former colonies.\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"How did the Hungarian Revolution of 1848 relate to broader European revolutionary movements?\",\n",
    "            \"expected_answer\": \"\"\"The Hungarian Revolution of 1848 represented one of the most significant and complex episodes within the \"Springtime of Peoples\"—the wave of revolutionary movements that swept across Europe in 1848-49. While sharing ideological currents with other European uprisings, Hungary's revolution developed distinctive characteristics due to its unique position within the Habsburg Empire, its constitutional traditions, and its complex ethnic composition. This revolution demonstrates both the interconnectedness of European nationalist movements and their divergent trajectories based on local conditions.\n",
    "\n",
    "The Hungarian Revolution emerged from the same fundamental causes that triggered uprisings across Europe—the rising power of liberalism and nationalism against autocratic systems, socioeconomic tensions as industrialization began transforming traditional societies, and the immediate catalyst of economic crisis. News of the February Revolution in Paris and subsequent uprising in Vienna in early March 1848 provided the spark for Hungarian action. Young radicals in Pest, led by poet Sándor Petőfi, articulated the \"Twelve Points\" demanding constitutional government, civil liberties, legal equality, and national autonomy within the Habsburg framework.\n",
    "\n",
    "Unlike many European revolutions, Hungary's initially achieved remarkable bloodless success. The Habsburg authorities, facing multiple crises throughout their domains, conceded to many Hungarian demands. Emperor Ferdinand V sanctioned the \"April Laws,\" which transformed Hungary from a semi-feudal system into a constitutional monarchy with its own ministry responsible to a parliamentary government. These reforms abolished serfdom, established civic equality, expanded suffrage, ended aristocratic tax exemptions, and created a Hungarian National Guard. The revolutionary government, led by moderate liberal Count Lajos Batthyány with Lajos Kossuth as its most dynamic voice, initially worked within a legal framework maintaining nominal allegiance to the Habsburg monarch as King of Hungary.\n",
    "\n",
    "Hungary's revolution developed unique characteristics related to its historical constitution and territorial claims. Unlike other revolutionary movements demanding new rights, Hungarian leaders framed their actions as defending ancient constitutional liberties dating back centuries but increasingly violated by Habsburg centralization. They sought not to dismantle existing legal structures but to adapt and modernize them while reasserting Hungary's historic autonomy. This included claiming authority over Transylvania, Croatia, and other territories within the traditional lands of the Crown of St. Stephen, setting the stage for ethnic conflicts that would complicate revolutionary aims.\n",
    "\n",
    "Indeed, Hungary's relationship with non-Magyar nationalities within its claimed territories created the revolution's most profound internal contradiction. While advancing liberal principles regarding individual rights, the revolutionary government maintained Magyar primacy in politics and administration, resisting demands for collective rights from Romanians, Slovaks, Serbs, and Croats. These minorities, constituting roughly 60% of Hungary's population, increasingly saw Hungarian nationalism as threatening their own national aspirations. By summer 1848, armed conflicts erupted between Hungarian forces and Croatian, Serbian, and Romanian movements, creating the paradoxical situation where Hungarian revolutionaries fighting for national liberation simultaneously suppressed other national movements within their borders.\n",
    "\n",
    "The Habsburg monarchy exploited these ethnic tensions in its counterrevolutionary strategy. After regaining control in Vienna by autumn 1848, imperial forces under Croatian Ban Josip Jelačić invaded Hungary. The revolutionary government, pushed toward increasingly radical positions by these existential threats, moved beyond its original constitutional framework. In December 1848, Ferdinand abdicated in favor of Franz Joseph, but the Hungarian Diet refused to recognize the new monarch. By April 1849, the Hungarian National Assembly issued the Declaration of Independence, formally dethroning the Habsburgs and establishing Kossuth as governor-president of an independent Hungarian state—moving from reform to complete revolution.\n",
    "\n",
    "The Hungarian revolutionary army, led by talented generals like Artúr Görgey, achieved remarkable military successes against imperial forces, controlling most of the country by spring 1849. This represented a level of revolutionary success unmatched elsewhere in Europe, where most uprisings had already been suppressed. However, this success prompted Tsar Nicholas I of Russia to intervene at Habsburg request. The entry of 200,000 Russian troops in June 1849 made Hungary's position untenable despite fierce resistance. By August 1849, Görgey surrendered at Világos, effectively ending the revolution.\n",
    "\n",
    "The aftermath was severe. Hungarian revolutionary leaders who didn't escape into exile faced execution, including Prime Minister Batthyány and thirteen generals hanged at Arad. Hungary lost its constitutional autonomy, subjected to direct rule from Vienna until the Austro-Hungarian Compromise of 1867 restored much of its self-governance. Nevertheless, the revolution left enduring legacies—the abolition of serfdom remained permanent, and the experience became central to Hungarian national identity and political culture.\n",
    "\n",
    "Within the broader European context, the Hungarian Revolution illustrated both the strength and limitations of the 1848 movements. It demonstrated liberalism's capacity to unite different social classes temporarily around shared political goals, yet also revealed the tensions between liberal universalism and exclusive nationalism. The revolution's ultimate failure highlighted the persistent strength of established powers and the disadvantages revolutionaries faced when divided by competing national claims. Unlike in Western Europe, where class conflict increasingly defined post-1848 politics, in Hungary and elsewhere in Central Europe, national questions remained paramount—a distinction that would shape these regions' divergent historical trajectories into the 20th century.\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What impact did the Franco regime have on Spanish society and culture?\",\n",
    "            \"expected_answer\": \"\"\"The Franco regime (1939-1975) profoundly transformed Spanish society and culture through nearly four decades of authoritarian rule, creating impacts that continue to reverberate in contemporary Spain. Following his victory in the Spanish Civil War, General Francisco Franco established a dictatorship characterized by nationalism, Catholic traditionalism, and anti-communism that sought to remake Spain according to conservative values while suppressing alternative visions. This extensive period of authoritarian control created deep cultural divides and collective traumas that Spain continues to navigate.\n",
    "\n",
    "The regime's social agenda centered on what Franco termed \"National-Catholicism\"—a fusion of Spanish nationalism with conservative Catholicism that permeated all aspects of society. The Catholic Church received extensive privileges, regaining control over education and marriage while Catholic rituals and symbols dominated public life. This represented a dramatic reversal of the Second Republic's secularization efforts and reinforced traditional gender roles and family structures. Women faced particular restrictions, with Franco-era legislation eliminating many Republican-era rights, promoting domesticity, and establishing legal subordination to male authority.\n",
    "\n",
    "Cultural expression underwent systematic control through extensive censorship. Literature, film, theater, and media required government approval, with works contradicting Catholic morality, questioning the regime, or expressing regional identities frequently banned. Many of Spain's most talented artists and intellectuals had fled into exile after the Civil War, creating a \"Two Spains\"—the official culture inside and the exiled culture abroad. Those who remained faced the choice between conformity, coded critique, or silence.\n",
    "\n",
    "Economic policy evolved significantly over the regime's duration. The early period (1939-1950s) pursued autarky—economic self-sufficiency through state intervention and protectionism—contributing to devastating poverty and hunger known as \"the years of hunger.\" By the late 1950s, technocrats associated with Opus Dei implemented liberalization measures, producing the \"Spanish Miracle\" of economic growth in the 1960s. This transformation from rural, agricultural society to urban, industrial one created massive internal migration, urbanization, and rising consumerism that gradually undermined traditional social structures the regime sought to preserve.\n",
    "\n",
    "The regime's approach to Spain's cultural diversity proved particularly consequential. Franco imposed Castilian Spanish as the sole official language, suppressing Catalan, Basque, and Galician languages and regional identities. This centralization policy, framed as promoting Spanish unity, instead fueled regionalist and separatist sentiments that would surge after Franco's death. Cultural practices associated with regional identities were either prohibited, folklorized for tourism, or reframed as expressions of a unified Spanish identity rather than distinct cultural traditions.\n",
    "\n",
    "Internationally, Spain experienced isolation following World War II due to Franco's Axis sympathies, though the Cold War eventually brought reintegration as Western powers prioritized anti-communism over democratic principles. This gradual international acceptance, alongside growing tourism from the 1960s, exposed Spaniards to foreign influences despite regime efforts to maintain cultural control. The resulting contrast between Spain's authoritarian system and neighboring democracies contributed to growing internal reform pressures.\n",
    "\n",
    "Youth culture emerged as a critical battleground. By the late Franco period, younger generations without direct Civil War experience increasingly rejected regime values. Universities became centers of resistance, while new musical forms, artistic movements, and social behaviors challenged traditional norms. This generational shift created what sociologists termed \"sociological Francoism\"—formal adherence to regime structures alongside growing private rejection of its values.\n",
    "\n",
    "The regime's final years witnessed both intensified repression and growing liberalization tensions. While the state executed political opponents as late as 1975, societal changes had already undermined many traditional structures. Franco's death initiated Spain's remarkable Transition to democracy, characterized by negotiated change rather than revolutionary rupture. This approach facilitated peaceful democratization but left many regime structures and personnel in place while establishing what some critics call a \"pact of forgetting\" regarding the dictatorship's crimes.\n",
    "\n",
    "Contemporary Spain continues grappling with Franco's legacy. The 2007 Historical Memory Law addressed Civil War and dictatorship victims, but debates persist regarding proper historical reckoning. Franco's 2019 exhumation from his monumental Valley of the Fallen mausoleum symbolized ongoing efforts to redefine national memory. Meanwhile, aspects of Francoist ideology have experienced partial revival through right-wing populism, while regional separatism—particularly in Catalonia—reflects unresolved tensions regarding Spain's national identity and governance.\n",
    "\n",
    "Ultimately, the Franco regime's impact on Spanish society and culture was profound and complex. While failing in its attempt to permanently reshape Spain according to its traditionalist vision, the dictatorship created enduring social divisions, psychological traumas, and institutional patterns that continue influencing contemporary Spain's political culture, regional relations, and collective memory—demonstrating how authoritarian systems produce consequences that outlast their formal existence.\"\"\"\n",
    "        }\n",
    "    ]\n",
    "    test_question=test_questions[0:2]\n",
    "    # Save to JSON file\n",
    "    with open('test_questions_with_answers.json', 'w') as f:\n",
    "        json.dump(test_questions, f, indent=2)\n",
    "    \n",
    "    print(f\"Created test questions JSON with {len(test_questions)} questions\")\n",
    "    return test_questions"
   ],
   "id": "10fc5b2f35214a71",
   "outputs": [],
   "execution_count": 209
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T05:33:02.874529Z",
     "start_time": "2025-05-13T05:24:27.146550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "os.environ[\"DEEPEVAL_USE_LOCAL_MODEL\"] = \"YES\"\n",
    "\n",
    "try:\n",
    "    with open('test_questions_with_answers.json', 'r') as f:\n",
    "        test_data = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    test_data = create_test_questions_json()\n",
    "\n",
    "# Extract just the questions for testing\n",
    "test_questions = [item[\"question\"] for item in test_data]\n",
    "\n",
    "# Test with different model\n",
    "print(\"Testing LM Studio model...\")\n",
    "lm_studio_results = test_lm_studio_model(test_questions, model_name=\"gemma-3-12b-it\")\n",
    "\n",
    "print(\"\\nTesting base Hugging Face model...\")\n",
    "base_hf_results = test_huggingface_model(test_questions, model_name=\"google/gemma-2-2b-it\", quantization=8)\n",
    "\n",
    "print(\"\\nTesting LoRA fine-tuned Hugging Face model...\")\n",
    "lora_path = \"../models/with_current_prompt/models_r16_8_final\"\n",
    "lora_hf_results = test_huggingface_model(\n",
    "    test_questions, \n",
    "    model_name=\"google/gemma-2-2b-it\", \n",
    "    quantization=8,\n",
    "    lora_weights=lora_path\n",
    ")\n",
    "\n",
    "#  Combine results for evaluation\n",
    "all_results = lm_studio_results + base_hf_results + lora_hf_results\n",
    "\n",
    "#  Evaluate with DeepEval\n",
    "print(\"\\nEvaluating results with DeepEval...\")\n",
    "evaluation_results = evaluate_with_deepeval(all_results, test_data)\n",
    "\n",
    "# Visualize the results\n",
    "visualize_evaluation_results(evaluation_results)\n",
    "\n",
    "#  Benchmark performance\n",
    "print(\"\\nBenchmarking performance...\")\n",
    "lm_studio_benchmark = benchmark_performance(\"Europe History\", model_source=\"lm_studio\")\n",
    "base_hf_benchmark = benchmark_performance(\"Europe History\", model_source=\"huggingface\")\n",
    "lora_hf_benchmark = benchmark_performance(\"Europe History\", model_source=\"huggingface\", lora_weights=lora_path)\n",
    "\n",
    "print(\"\\nLM Studio benchmark results:\")\n",
    "for metric, value in lm_studio_benchmark.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{metric}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"{metric}: {value}\")\n",
    "\n",
    "print(\"\\nBase Hugging Face benchmark results:\")\n",
    "for metric, value in base_hf_benchmark.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{metric}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"{metric}: {value}\")\n",
    "\n",
    "print(\"\\nLoRA Hugging Face benchmark results:\")\n",
    "for metric, value in lora_hf_benchmark.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{metric}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"{metric}: {value}\")"
   ],
   "id": "5e76cdf0b1193335",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing LM Studio model...\n",
      "Testing LM Studio model: gemma-3-12b-it\n",
      "Vector store loaded with 4168 documents\n",
      "[1/12] Processing: What were the main causes of World War I?\n",
      "Response: The main causes of World War I can be summarized as follows, based on the provided information:\n",
      "\n",
      "*  ...\n",
      "Time: 34.84s (retrieval: 0.10s, generation: 32.61s)\n",
      "[2/12] Processing: How did the Industrial Revolution change society?\n",
      "Response: The Industrial Revolution brought about profound changes to society, fundamentally reshaping its str...\n",
      "Time: 42.09s (retrieval: 0.06s, generation: 39.68s)\n",
      "[3/12] Processing: What was the significance of the French Revolution?\n",
      "Response: The French Revolution (1789-1799) was profoundly significant for France and Europe, marking a period...\n",
      "Time: 35.52s (retrieval: 0.08s, generation: 33.38s)\n",
      "[4/12] Processing: How did the Cold War affect international relations?\n",
      "Response: The Cold War profoundly reshaped international relations, and the provided chunks of information ill...\n",
      "Time: 63.71s (retrieval: 0.07s, generation: 61.56s)\n",
      "[5/12] Processing: How did Romania transition from communism to democracy in 1989?\n",
      "Response: Romania's transition from communism to democracy in 1989 was a swift and violent revolution triggere...\n",
      "Time: 52.47s (retrieval: 0.06s, generation: 50.34s)\n",
      "[6/12] Processing: What were the key events of the Spanish Civil War (1936-1939)?\n",
      "Response: Here's a breakdown of the key events of the Spanish Civil War (1936-1939), based on the provided inf...\n",
      "Time: 45.55s (retrieval: 0.02s, generation: 43.46s)\n",
      "[7/12] Processing: How did the unification of Germany in the 19th century reshape European politics?\n",
      "Response: The unification of Germany in the mid-19th century fundamentally reshaped European politics, creatin...\n",
      "Time: 57.35s (retrieval: 0.08s, generation: 55.18s)\n",
      "[8/12] Processing: What was the significance of the Hungarian Revolution of 1956?\n",
      "Response: The significance of the Hungarian Revolution of 1956 lies in its bold challenge to Soviet control ov...\n",
      "Time: 43.98s (retrieval: 0.08s, generation: 41.82s)\n",
      "[9/12] Processing: How did Bulgaria's experience under Ottoman rule shape its national identity?\n",
      "Response: Bulgaria’s experience under Ottoman rule profoundly shaped its national identity, forging a complex ...\n",
      "Time: 48.23s (retrieval: 0.08s, generation: 46.06s)\n",
      "[10/12] Processing: What role did Spain play in the European colonization of the Americas?\n",
      "Response: Spain played a **pivotal and leading role** in the European colonization of the Americas. Here's a b...\n",
      "Time: 38.10s (retrieval: 0.01s, generation: 36.02s)\n",
      "[11/12] Processing: How did the Hungarian Revolution of 1848 relate to broader European revolutionary movements?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[210]\u001B[39m\u001B[32m, line 20\u001B[39m\n\u001B[32m     18\u001B[39m \u001B[38;5;66;03m# 3. Test with different model\u001B[39;00m\n\u001B[32m     19\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mTesting LM Studio model...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m20\u001B[39m lm_studio_results = test_lm_studio_model(test_questions, model_name=\u001B[33m\"\u001B[39m\u001B[33mgemma-3-12b-it\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     22\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mTesting base Hugging Face model...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     23\u001B[39m base_hf_results = test_huggingface_model(test_questions, model_name=\u001B[33m\"\u001B[39m\u001B[33mgoogle/gemma-2-2b-it\u001B[39m\u001B[33m\"\u001B[39m, quantization=\u001B[32m8\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[204]\u001B[39m\u001B[32m, line 61\u001B[39m, in \u001B[36mtest_lm_studio_model\u001B[39m\u001B[34m(test_questions, model_name)\u001B[39m\n\u001B[32m     59\u001B[39m \u001B[38;5;66;03m# Generate response\u001B[39;00m\n\u001B[32m     60\u001B[39m generation_start = time.time()\n\u001B[32m---> \u001B[39m\u001B[32m61\u001B[39m response = model.invoke(chat_history)\n\u001B[32m     62\u001B[39m generation_time = time.time() - generation_start\n\u001B[32m     64\u001B[39m \u001B[38;5;66;03m# Calculate total time\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:369\u001B[39m, in \u001B[36mBaseChatModel.invoke\u001B[39m\u001B[34m(self, input, config, stop, **kwargs)\u001B[39m\n\u001B[32m    357\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m    358\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34minvoke\u001B[39m(\n\u001B[32m    359\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    364\u001B[39m     **kwargs: Any,\n\u001B[32m    365\u001B[39m ) -> BaseMessage:\n\u001B[32m    366\u001B[39m     config = ensure_config(config)\n\u001B[32m    367\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[32m    368\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mChatGeneration\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m--> \u001B[39m\u001B[32m369\u001B[39m         \u001B[38;5;28mself\u001B[39m.generate_prompt(\n\u001B[32m    370\u001B[39m             [\u001B[38;5;28mself\u001B[39m._convert_input(\u001B[38;5;28minput\u001B[39m)],\n\u001B[32m    371\u001B[39m             stop=stop,\n\u001B[32m    372\u001B[39m             callbacks=config.get(\u001B[33m\"\u001B[39m\u001B[33mcallbacks\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m    373\u001B[39m             tags=config.get(\u001B[33m\"\u001B[39m\u001B[33mtags\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m    374\u001B[39m             metadata=config.get(\u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m    375\u001B[39m             run_name=config.get(\u001B[33m\"\u001B[39m\u001B[33mrun_name\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m    376\u001B[39m             run_id=config.pop(\u001B[33m\"\u001B[39m\u001B[33mrun_id\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[32m    377\u001B[39m             **kwargs,\n\u001B[32m    378\u001B[39m         ).generations[\u001B[32m0\u001B[39m][\u001B[32m0\u001B[39m],\n\u001B[32m    379\u001B[39m     ).message\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:946\u001B[39m, in \u001B[36mBaseChatModel.generate_prompt\u001B[39m\u001B[34m(self, prompts, stop, callbacks, **kwargs)\u001B[39m\n\u001B[32m    937\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m    938\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mgenerate_prompt\u001B[39m(\n\u001B[32m    939\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    943\u001B[39m     **kwargs: Any,\n\u001B[32m    944\u001B[39m ) -> LLMResult:\n\u001B[32m    945\u001B[39m     prompt_messages = [p.to_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[32m--> \u001B[39m\u001B[32m946\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:765\u001B[39m, in \u001B[36mBaseChatModel.generate\u001B[39m\u001B[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[39m\n\u001B[32m    762\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(input_messages):\n\u001B[32m    763\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    764\u001B[39m         results.append(\n\u001B[32m--> \u001B[39m\u001B[32m765\u001B[39m             \u001B[38;5;28mself\u001B[39m._generate_with_cache(\n\u001B[32m    766\u001B[39m                 m,\n\u001B[32m    767\u001B[39m                 stop=stop,\n\u001B[32m    768\u001B[39m                 run_manager=run_managers[i] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    769\u001B[39m                 **kwargs,\n\u001B[32m    770\u001B[39m             )\n\u001B[32m    771\u001B[39m         )\n\u001B[32m    772\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    773\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1011\u001B[39m, in \u001B[36mBaseChatModel._generate_with_cache\u001B[39m\u001B[34m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m   1009\u001B[39m     result = generate_from_stream(\u001B[38;5;28miter\u001B[39m(chunks))\n\u001B[32m   1010\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m inspect.signature(\u001B[38;5;28mself\u001B[39m._generate).parameters.get(\u001B[33m\"\u001B[39m\u001B[33mrun_manager\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1011\u001B[39m     result = \u001B[38;5;28mself\u001B[39m._generate(\n\u001B[32m   1012\u001B[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001B[32m   1013\u001B[39m     )\n\u001B[32m   1014\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1015\u001B[39m     result = \u001B[38;5;28mself\u001B[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:991\u001B[39m, in \u001B[36mBaseChatOpenAI._generate\u001B[39m\u001B[34m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m    989\u001B[39m     generation_info = {\u001B[33m\"\u001B[39m\u001B[33mheaders\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mdict\u001B[39m(raw_response.headers)}\n\u001B[32m    990\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m991\u001B[39m     response = \u001B[38;5;28mself\u001B[39m.client.create(**payload)\n\u001B[32m    992\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._create_chat_result(response, generation_info)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\openai\\_utils\\_utils.py:287\u001B[39m, in \u001B[36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    285\u001B[39m             msg = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[32m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    286\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[32m--> \u001B[39m\u001B[32m287\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m func(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:925\u001B[39m, in \u001B[36mCompletions.create\u001B[39m\u001B[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001B[39m\n\u001B[32m    882\u001B[39m \u001B[38;5;129m@required_args\u001B[39m([\u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m], [\u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mstream\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m    883\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcreate\u001B[39m(\n\u001B[32m    884\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    922\u001B[39m     timeout: \u001B[38;5;28mfloat\u001B[39m | httpx.Timeout | \u001B[38;5;28;01mNone\u001B[39;00m | NotGiven = NOT_GIVEN,\n\u001B[32m    923\u001B[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001B[32m    924\u001B[39m     validate_response_format(response_format)\n\u001B[32m--> \u001B[39m\u001B[32m925\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._post(\n\u001B[32m    926\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m/chat/completions\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    927\u001B[39m         body=maybe_transform(\n\u001B[32m    928\u001B[39m             {\n\u001B[32m    929\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m: messages,\n\u001B[32m    930\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m: model,\n\u001B[32m    931\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33maudio\u001B[39m\u001B[33m\"\u001B[39m: audio,\n\u001B[32m    932\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mfrequency_penalty\u001B[39m\u001B[33m\"\u001B[39m: frequency_penalty,\n\u001B[32m    933\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mfunction_call\u001B[39m\u001B[33m\"\u001B[39m: function_call,\n\u001B[32m    934\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mfunctions\u001B[39m\u001B[33m\"\u001B[39m: functions,\n\u001B[32m    935\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mlogit_bias\u001B[39m\u001B[33m\"\u001B[39m: logit_bias,\n\u001B[32m    936\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mlogprobs\u001B[39m\u001B[33m\"\u001B[39m: logprobs,\n\u001B[32m    937\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmax_completion_tokens\u001B[39m\u001B[33m\"\u001B[39m: max_completion_tokens,\n\u001B[32m    938\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmax_tokens\u001B[39m\u001B[33m\"\u001B[39m: max_tokens,\n\u001B[32m    939\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: metadata,\n\u001B[32m    940\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmodalities\u001B[39m\u001B[33m\"\u001B[39m: modalities,\n\u001B[32m    941\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mn\u001B[39m\u001B[33m\"\u001B[39m: n,\n\u001B[32m    942\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mparallel_tool_calls\u001B[39m\u001B[33m\"\u001B[39m: parallel_tool_calls,\n\u001B[32m    943\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mprediction\u001B[39m\u001B[33m\"\u001B[39m: prediction,\n\u001B[32m    944\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mpresence_penalty\u001B[39m\u001B[33m\"\u001B[39m: presence_penalty,\n\u001B[32m    945\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mreasoning_effort\u001B[39m\u001B[33m\"\u001B[39m: reasoning_effort,\n\u001B[32m    946\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mresponse_format\u001B[39m\u001B[33m\"\u001B[39m: response_format,\n\u001B[32m    947\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mseed\u001B[39m\u001B[33m\"\u001B[39m: seed,\n\u001B[32m    948\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mservice_tier\u001B[39m\u001B[33m\"\u001B[39m: service_tier,\n\u001B[32m    949\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mstop\u001B[39m\u001B[33m\"\u001B[39m: stop,\n\u001B[32m    950\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mstore\u001B[39m\u001B[33m\"\u001B[39m: store,\n\u001B[32m    951\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mstream\u001B[39m\u001B[33m\"\u001B[39m: stream,\n\u001B[32m    952\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mstream_options\u001B[39m\u001B[33m\"\u001B[39m: stream_options,\n\u001B[32m    953\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtemperature\u001B[39m\u001B[33m\"\u001B[39m: temperature,\n\u001B[32m    954\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtool_choice\u001B[39m\u001B[33m\"\u001B[39m: tool_choice,\n\u001B[32m    955\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtools\u001B[39m\u001B[33m\"\u001B[39m: tools,\n\u001B[32m    956\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtop_logprobs\u001B[39m\u001B[33m\"\u001B[39m: top_logprobs,\n\u001B[32m    957\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtop_p\u001B[39m\u001B[33m\"\u001B[39m: top_p,\n\u001B[32m    958\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33muser\u001B[39m\u001B[33m\"\u001B[39m: user,\n\u001B[32m    959\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mweb_search_options\u001B[39m\u001B[33m\"\u001B[39m: web_search_options,\n\u001B[32m    960\u001B[39m             },\n\u001B[32m    961\u001B[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001B[32m    962\u001B[39m             \u001B[38;5;28;01mif\u001B[39;00m stream\n\u001B[32m    963\u001B[39m             \u001B[38;5;28;01melse\u001B[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001B[32m    964\u001B[39m         ),\n\u001B[32m    965\u001B[39m         options=make_request_options(\n\u001B[32m    966\u001B[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001B[32m    967\u001B[39m         ),\n\u001B[32m    968\u001B[39m         cast_to=ChatCompletion,\n\u001B[32m    969\u001B[39m         stream=stream \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m    970\u001B[39m         stream_cls=Stream[ChatCompletionChunk],\n\u001B[32m    971\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\openai\\_base_client.py:1239\u001B[39m, in \u001B[36mSyncAPIClient.post\u001B[39m\u001B[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[39m\n\u001B[32m   1225\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpost\u001B[39m(\n\u001B[32m   1226\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1227\u001B[39m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1234\u001B[39m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1235\u001B[39m ) -> ResponseT | _StreamT:\n\u001B[32m   1236\u001B[39m     opts = FinalRequestOptions.construct(\n\u001B[32m   1237\u001B[39m         method=\u001B[33m\"\u001B[39m\u001B[33mpost\u001B[39m\u001B[33m\"\u001B[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001B[32m   1238\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1239\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28mself\u001B[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\openai\\_base_client.py:969\u001B[39m, in \u001B[36mSyncAPIClient.request\u001B[39m\u001B[34m(self, cast_to, options, stream, stream_cls)\u001B[39m\n\u001B[32m    967\u001B[39m response = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    968\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m969\u001B[39m     response = \u001B[38;5;28mself\u001B[39m._client.send(\n\u001B[32m    970\u001B[39m         request,\n\u001B[32m    971\u001B[39m         stream=stream \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._should_stream_response_body(request=request),\n\u001B[32m    972\u001B[39m         **kwargs,\n\u001B[32m    973\u001B[39m     )\n\u001B[32m    974\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m httpx.TimeoutException \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[32m    975\u001B[39m     log.debug(\u001B[33m\"\u001B[39m\u001B[33mEncountered httpx.TimeoutException\u001B[39m\u001B[33m\"\u001B[39m, exc_info=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\httpx\\_client.py:914\u001B[39m, in \u001B[36mClient.send\u001B[39m\u001B[34m(self, request, stream, auth, follow_redirects)\u001B[39m\n\u001B[32m    906\u001B[39m follow_redirects = (\n\u001B[32m    907\u001B[39m     \u001B[38;5;28mself\u001B[39m.follow_redirects\n\u001B[32m    908\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(follow_redirects, UseClientDefault)\n\u001B[32m    909\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m follow_redirects\n\u001B[32m    910\u001B[39m )\n\u001B[32m    912\u001B[39m auth = \u001B[38;5;28mself\u001B[39m._build_request_auth(request, auth)\n\u001B[32m--> \u001B[39m\u001B[32m914\u001B[39m response = \u001B[38;5;28mself\u001B[39m._send_handling_auth(\n\u001B[32m    915\u001B[39m     request,\n\u001B[32m    916\u001B[39m     auth=auth,\n\u001B[32m    917\u001B[39m     follow_redirects=follow_redirects,\n\u001B[32m    918\u001B[39m     history=[],\n\u001B[32m    919\u001B[39m )\n\u001B[32m    920\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    921\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stream:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\httpx\\_client.py:942\u001B[39m, in \u001B[36mClient._send_handling_auth\u001B[39m\u001B[34m(self, request, auth, follow_redirects, history)\u001B[39m\n\u001B[32m    939\u001B[39m request = \u001B[38;5;28mnext\u001B[39m(auth_flow)\n\u001B[32m    941\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m942\u001B[39m     response = \u001B[38;5;28mself\u001B[39m._send_handling_redirects(\n\u001B[32m    943\u001B[39m         request,\n\u001B[32m    944\u001B[39m         follow_redirects=follow_redirects,\n\u001B[32m    945\u001B[39m         history=history,\n\u001B[32m    946\u001B[39m     )\n\u001B[32m    947\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    948\u001B[39m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\httpx\\_client.py:979\u001B[39m, in \u001B[36mClient._send_handling_redirects\u001B[39m\u001B[34m(self, request, follow_redirects, history)\u001B[39m\n\u001B[32m    976\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._event_hooks[\u001B[33m\"\u001B[39m\u001B[33mrequest\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m    977\u001B[39m     hook(request)\n\u001B[32m--> \u001B[39m\u001B[32m979\u001B[39m response = \u001B[38;5;28mself\u001B[39m._send_single_request(request)\n\u001B[32m    980\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    981\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._event_hooks[\u001B[33m\"\u001B[39m\u001B[33mresponse\u001B[39m\u001B[33m\"\u001B[39m]:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\httpx\\_client.py:1015\u001B[39m, in \u001B[36mClient._send_single_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m   1010\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m   1011\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAttempted to send an async request with a sync Client instance.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1012\u001B[39m     )\n\u001B[32m   1014\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request=request):\n\u001B[32m-> \u001B[39m\u001B[32m1015\u001B[39m     response = transport.handle_request(request)\n\u001B[32m   1017\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response.stream, SyncByteStream)\n\u001B[32m   1019\u001B[39m response.request = request\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\httpx\\_transports\\default.py:233\u001B[39m, in \u001B[36mHTTPTransport.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    220\u001B[39m req = httpcore.Request(\n\u001B[32m    221\u001B[39m     method=request.method,\n\u001B[32m    222\u001B[39m     url=httpcore.URL(\n\u001B[32m   (...)\u001B[39m\u001B[32m    230\u001B[39m     extensions=request.extensions,\n\u001B[32m    231\u001B[39m )\n\u001B[32m    232\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[32m--> \u001B[39m\u001B[32m233\u001B[39m     resp = \u001B[38;5;28mself\u001B[39m._pool.handle_request(req)\n\u001B[32m    235\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(resp.stream, typing.Iterable)\n\u001B[32m    237\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m Response(\n\u001B[32m    238\u001B[39m     status_code=resp.status,\n\u001B[32m    239\u001B[39m     headers=resp.headers,\n\u001B[32m    240\u001B[39m     stream=ResponseStream(resp.stream),\n\u001B[32m    241\u001B[39m     extensions=resp.extensions,\n\u001B[32m    242\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:268\u001B[39m, in \u001B[36mConnectionPool.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    266\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ShieldCancellation():\n\u001B[32m    267\u001B[39m         \u001B[38;5;28mself\u001B[39m.response_closed(status)\n\u001B[32m--> \u001B[39m\u001B[32m268\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc\n\u001B[32m    269\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    270\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:251\u001B[39m, in \u001B[36mConnectionPool.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    248\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m exc\n\u001B[32m    250\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m251\u001B[39m     response = connection.handle_request(request)\n\u001B[32m    252\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m ConnectionNotAvailable:\n\u001B[32m    253\u001B[39m     \u001B[38;5;66;03m# The ConnectionNotAvailable exception is a special case, that\u001B[39;00m\n\u001B[32m    254\u001B[39m     \u001B[38;5;66;03m# indicates we need to retry the request on a new connection.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    258\u001B[39m     \u001B[38;5;66;03m# might end up as an HTTP/2 connection, but which actually ends\u001B[39;00m\n\u001B[32m    259\u001B[39m     \u001B[38;5;66;03m# up as HTTP/1.1.\u001B[39;00m\n\u001B[32m    260\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pool_lock:\n\u001B[32m    261\u001B[39m         \u001B[38;5;66;03m# Maintain our position in the request queue, but reset the\u001B[39;00m\n\u001B[32m    262\u001B[39m         \u001B[38;5;66;03m# status so that the request becomes queued again.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001B[39m, in \u001B[36mHTTPConnection.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    100\u001B[39m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._connection.is_available():\n\u001B[32m    101\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m ConnectionNotAvailable()\n\u001B[32m--> \u001B[39m\u001B[32m103\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._connection.handle_request(request)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:133\u001B[39m, in \u001B[36mHTTP11Connection.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    131\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m Trace(\u001B[33m\"\u001B[39m\u001B[33mresponse_closed\u001B[39m\u001B[33m\"\u001B[39m, logger, request) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[32m    132\u001B[39m         \u001B[38;5;28mself\u001B[39m._response_closed()\n\u001B[32m--> \u001B[39m\u001B[32m133\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:111\u001B[39m, in \u001B[36mHTTP11Connection.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    101\u001B[39m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[32m    103\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m Trace(\n\u001B[32m    104\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mreceive_response_headers\u001B[39m\u001B[33m\"\u001B[39m, logger, request, kwargs\n\u001B[32m    105\u001B[39m ) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[32m    106\u001B[39m     (\n\u001B[32m    107\u001B[39m         http_version,\n\u001B[32m    108\u001B[39m         status,\n\u001B[32m    109\u001B[39m         reason_phrase,\n\u001B[32m    110\u001B[39m         headers,\n\u001B[32m--> \u001B[39m\u001B[32m111\u001B[39m     ) = \u001B[38;5;28mself\u001B[39m._receive_response_headers(**kwargs)\n\u001B[32m    112\u001B[39m     trace.return_value = (\n\u001B[32m    113\u001B[39m         http_version,\n\u001B[32m    114\u001B[39m         status,\n\u001B[32m    115\u001B[39m         reason_phrase,\n\u001B[32m    116\u001B[39m         headers,\n\u001B[32m    117\u001B[39m     )\n\u001B[32m    119\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m Response(\n\u001B[32m    120\u001B[39m     status=status,\n\u001B[32m    121\u001B[39m     headers=headers,\n\u001B[32m   (...)\u001B[39m\u001B[32m    127\u001B[39m     },\n\u001B[32m    128\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:176\u001B[39m, in \u001B[36mHTTP11Connection._receive_response_headers\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    173\u001B[39m timeout = timeouts.get(\u001B[33m\"\u001B[39m\u001B[33mread\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m    175\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m176\u001B[39m     event = \u001B[38;5;28mself\u001B[39m._receive_event(timeout=timeout)\n\u001B[32m    177\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(event, h11.Response):\n\u001B[32m    178\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:212\u001B[39m, in \u001B[36mHTTP11Connection._receive_event\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    209\u001B[39m     event = \u001B[38;5;28mself\u001B[39m._h11_state.next_event()\n\u001B[32m    211\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m event \u001B[38;5;129;01mis\u001B[39;00m h11.NEED_DATA:\n\u001B[32m--> \u001B[39m\u001B[32m212\u001B[39m     data = \u001B[38;5;28mself\u001B[39m._network_stream.read(\n\u001B[32m    213\u001B[39m         \u001B[38;5;28mself\u001B[39m.READ_NUM_BYTES, timeout=timeout\n\u001B[32m    214\u001B[39m     )\n\u001B[32m    216\u001B[39m     \u001B[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001B[39;00m\n\u001B[32m    217\u001B[39m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m    218\u001B[39m     \u001B[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    222\u001B[39m     \u001B[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001B[39;00m\n\u001B[32m    223\u001B[39m     \u001B[38;5;66;03m# it as a ConnectError.\u001B[39;00m\n\u001B[32m    224\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m data == \u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\httpcore\\_backends\\sync.py:126\u001B[39m, in \u001B[36mSyncStream.read\u001B[39m\u001B[34m(self, max_bytes, timeout)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m map_exceptions(exc_map):\n\u001B[32m    125\u001B[39m     \u001B[38;5;28mself\u001B[39m._sock.settimeout(timeout)\n\u001B[32m--> \u001B[39m\u001B[32m126\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sock.recv(max_bytes)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 210
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T05:33:02.881533300Z",
     "start_time": "2025-05-13T04:09:05.928055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "# run a prompt to see how the models generate\n",
    "prompt= \"What happend in the 20th century in Romania?\"\n",
    "# run the prompt on the models\n",
    "lm_studio_model = \"gemma-3-12b-it\"\n",
    "\n",
    "\n",
    "def run_lm_studio_model(question, model_name):\n",
    "    embedding = EmbeddingModel()\n",
    "    vectordb = Chroma(\n",
    "        embedding_function=embedding,\n",
    "        persist_directory=DATABASE_PATH,\n",
    "    )\n",
    "    print(f\"Vector store loaded with {vectordb._collection.count()} documents\")\n",
    "    \n",
    "    # Initialize LM Studio model\n",
    "    model = ChatOpenAI(\n",
    "        base_url=LM_STUDIO_URL,\n",
    "        api_key=\"lm-studio\",\n",
    "        temperature=0.3,\n",
    "        model=model_name,\n",
    "    )\n",
    "        \n",
    "    chat_history = [{\"role\": \"user\", \"content\": question}]\n",
    "    \n",
    "    \n",
    "    embedded_query = embedding.embed_query(question)\n",
    "    retrieved_chunks = vectordb.similarity_search_by_vector(embedded_query)\n",
    "    \n",
    "    formatted_chunks = \"\"\n",
    "    for chunk in retrieved_chunks:\n",
    "        formatted_chunks += f\"- {chunk.page_content}\\n\"\n",
    "    \n",
    "    # Augment prompt\n",
    "    augmented_prompt = f\"{question}\\nHere are some chunks of information that could help you, they might be out of order. You should use them only if they are relevant to my question.\\nThe chunks are:{formatted_chunks}\"\n",
    "    chat_history[0][\"content\"] = augmented_prompt\n",
    "    \n",
    "    response = model.invoke(chat_history)\n",
    "    return response\n",
    "\n",
    "def run_hf_model(question,model_name=\"google/gemma-2-2b-it\", quantization=8, lora_weights=None):\n",
    "    load_huggingface_model(model_name=model_name, quantization=quantization, lora_weights=lora_weights)\n",
    "    \n",
    "    global llm, tokenizer\n",
    "    \n",
    "    # Initialize embedding model and database\n",
    "    embedding = EmbeddingModel()\n",
    "    vectordb = Chroma(\n",
    "        embedding_function=embedding,\n",
    "        persist_directory=DATABASE_PATH,\n",
    "    )\n",
    "    print(f\"Vector store loaded with {vectordb._collection.count()} documents\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    embedded_query = embedding.embed_query(question)\n",
    "    retrieved_chunks = vectordb.similarity_search_by_vector(embedded_query)\n",
    "    \n",
    "    # Format context\n",
    "    context = \"\"\n",
    "    for chunk in retrieved_chunks:\n",
    "        context += f\"{chunk.page_content}\\n\\n\"\n",
    "    \n",
    "    prompt = f\"\"\"### Instruction: \n",
    "You are a knowledgeable history tutor. Answer the following question accurately based on the provided historical context.\n",
    "Use ONLY the information provided in the context. If you don't know, say \"I don't have enough information to answer that.\"\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\n",
    "\"\"\"\n",
    "    \n",
    "    # Generate response\n",
    "    try:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(llm.device)\n",
    "        outputs = llm.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.3,\n",
    "            do_sample=True\n",
    "        )\n",
    "        response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        response_text = f\"Error: {str(e)}\"\n",
    "    \n",
    "    # Calculate total time\n",
    "    return response_text\n",
    "\n",
    "\n",
    "lm_studio_response = run_lm_studio_model(prompt, model_name=lm_studio_model)\n",
    "huggin_face_response= run_hf_model(prompt, model_name=\"google/gemma-2-2b-it\", quantization=8, lora_weights=None)\n",
    "lora_path = \"../models/with_current_prompt/models_r16_8_final\"\n",
    "hugging_face_response_lora= run_hf_model(prompt, model_name=\"google/gemma-2-2b-it\", quantization=8, lora_weights=lora_path)\n",
    "\n",
    "print(\"\\nLM Studio response:\")\n",
    "print(lm_studio_response)\n",
    "print(\"\\nHugging Face response:\")\n",
    "print(huggin_face_response)\n",
    "print(\"\\nHugging Face response with LoRA:\")\n",
    "print(hugging_face_response_lora)\n",
    "\n",
    "\n"
   ],
   "id": "28699e69076a291f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store loaded with 4168 documents\n",
      "Model deleted\n",
      "Tokenizer deleted\n",
      "Memory cleaned up\n",
      "Optimizing model configuration...\n",
      "CUDA available: NVIDIA GeForce RTX 3080 Laptop GPU\n",
      "CUDA memory: 8.59 GB\n",
      "Using balanced settings (8-bit quantization)\n",
      "Loading model...\n",
      "Forcing model to load on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loaded successfully\n",
      "Model loaded on: cuda:0\n",
      "Vector store loaded with 4168 documents\n",
      "Model deleted\n",
      "Tokenizer deleted\n",
      "Memory cleaned up\n",
      "Optimizing model configuration...\n",
      "CUDA available: NVIDIA GeForce RTX 3080 Laptop GPU\n",
      "CUDA memory: 8.59 GB\n",
      "Using balanced settings (8-bit quantization)\n",
      "Loading model...\n",
      "Forcing model to load on GPU\n",
      "Loading base model with LoRA adapter from ../models/with_current_prompt/models_r16_8_final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapter loaded successfully\n",
      "Model loaded on: cuda:0\n",
      "Vector store loaded with 4168 documents\n",
      "\n",
      "LM Studio response:\n",
      "content='Here\\'s a breakdown of what happened in Romania during the 20th century, based on the provided information:\\n\\n**Early 20th Century (Up to World War I):**\\n\\n*   **Territorial Disputes:** A significant conflict with Bulgaria over the Dobruja region led to war in 1913. Romania emerged victorious, gaining Southern Dobruja through the Treaty of Bucharest. This dispute continued and was resolved with the Treaty of Craiova in 1940, which involved a population exchange.\\n*   **Political Instability:** Following World War I, Romania experienced significant political instability. The constitution granted the king considerable power, leading to frequent government changes (over 25 in one decade). This period was marked by the rise of antisemitic, ultra-nationalist, and quasi-fascist parties.\\n*   **Economic Shifts:** French economic and political influence initially dominated, but Germany\\'s influence grew significantly during the 1930s. While industry expanded, agriculture remained the primary employer for most Romanians.\\n\\n**Interwar Period (1920s - 1930s):**\\n\\n*   **Political Decline:** Romania’s democracy gradually deteriorated towards a fascist dictatorship as the 1930s progressed.\\n*   **Economic Recovery:** The Romanian economy recovered and industry grew significantly in the mid-1930s.\\n\\n**World War II & Immediate Post-War (1940s):**\\n\\n*   **Alignment with Nazi Germany:** Romania fought on the side of Nazi Germany during World War II.\\n*   **Territorial Losses:**  The Treaty of Craiova in 1940 resolved the Dobruja dispute, but also involved territorial concessions.\\n*   **Soviet Occupation & Communist Takeover:** Following World War II, Soviet occupation facilitated the rise of the Romanian Communist Party. This led to the forced abdication of the King and the establishment of a communist people\\'s republic in 1947.\\n\\n**Cold War Era (1950s - 1980s):**\\n\\n*   **Soviet Control:** Romania was under military and economic control of the Soviet Union until the late 1950s. Resources were drained through \"SovRom\" agreements, which masked Soviet exploitation.\\n*   **Gheorghe Gheorghiu-Dej\\'s Rule:** Gheorghe Gheorghiu-Dej led Romania from 1948 until his death in 1965 as the First Secretary of the Romanian Workers’ Party. The communist regime was formalized with a new constitution in 1948.\\n*   **Formal Departure from Soviet Sphere:** In 1964, Romania formally left the Soviet sphere of influence.\\n\\n**Late 20th Century (1989):**\\n\\n*   **Collapse of Communism:** The communist regime collapsed in 1989, marking a significant turning point in Romanian history.\\n\\n\\n\\nThis overview highlights the major political, economic, and territorial shifts that shaped Romania throughout the tumultuous 20th century.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 647, 'prompt_tokens': 621, 'total_tokens': 1268, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gemma-3-12b-it', 'system_fingerprint': 'gemma-3-12b-it', 'id': 'chatcmpl-2etndrnyu942nyrkfwbbdt', 'finish_reason': 'stop', 'logprobs': None} id='run-92868bdf-27d0-4e3d-bfa0-4152945a069d-0' usage_metadata={'input_tokens': 621, 'output_tokens': 647, 'total_tokens': 1268, 'input_token_details': {}, 'output_token_details': {}}\n",
      "\n",
      "Hugging Face response:\n",
      "### Instruction: \n",
      "You are a knowledgeable history tutor. Answer the following question accurately based on the provided historical context.\n",
      "Use ONLY the information provided in the context. If you don't know, say \"I don't have enough information to answer that.\"\n",
      "\n",
      "### Context:\n",
      "Jiului and the strike in the Grivița railroad workshops. In the mid-1930s, the Romanian economy recovered and the industry grew significantly, although about 80% of Romanians were still employed in agriculture. French economic and political influence was predominant in the early 1920s but then Germany became more dominant, especially in the 1930s. As the 1930s progressed, Romania's already shaky democracy slowly deteriorated toward fascist dictatorship. The constitution of 1923 gave the king free rein to dissolve parliament and call elections at will; as a result, Romania was to experience over 25 governments in a single decade. Increasingly, these governments were dominated by a number of antisemitic, ultra-nationalist, and mostly at least quasi-fascist parties.\n",
      "\n",
      "In Romania proper, Soviet occupation following World War II facilitated the rise of the Communist Party as the main political force, leading ultimately to the forced abdication of the King and the establishment of a single-party people's republic in 1947. Romania was proclaimed a people's republic and remained under military and economic control of the Soviet Union until the late 1950s. During this period, Romania's resources were drained by the \"SovRom\" agreements; mixed Soviet-Romanian companies were established to mask the Soviet Union's looting of Romania. Romania's leader from 1948 to his death in 1965 was Gheorghe Gheorghiu-Dej, the First Secretary of the Romanian Workers' Party. The Communist regime was formalised with the constitution of 13 April 1948.\n",
      "\n",
      "against Romania, the capture of Bucharest and the signing of the Treaty of Bucharest. During the 20th century, Bulgaria and Romania both fought on the side of Nazi Germany during the Second World War. In the cold war, both countries became communist states under the influence of the Soviet Union, but Romania formally left the sphere in 1964. The communist regimes of both countries ultimately collapsed in 1989.\n",
      "\n",
      "During the first half of the 20th century, Romania and Bulgaria had a serious conflict over the Dobruja region. This dispute, while now largely forgotten, escalated into all out war in 1913. Romania participated in the Second Balkan War against Bulgaria. According to the Treaty of Bucharest concluded after the Bulgarian defeat, Bulgaria had to hand over Southern Dobruja region to Romania. The territorial dispute between the two countries ended with the Treaty of Craiova in 1940. After the treaty, the two countries carried out a population exchange in the affected areas.\n",
      "\n",
      "\n",
      "\n",
      "### Question:\n",
      "What happend in the 20th century in Romania?\n",
      "\n",
      "### Answer:\n",
      "During the first half of the 20th century, Romania and Bulgaria had a serious conflict over the Dobruja region. This dispute, while now largely forgotten, escalated into all out war in 1913. Romania participated in the Second Balkan War against Bulgaria. According to the Treaty of Bucharest concluded after the Bulgarian defeat, Bulgaria had to hand over Southern Dobruja region to Romania. The territorial dispute between the two countries ended with the Treaty of Craiova in 1940. After the treaty, the two countries carried out a population exchange in the affected areas. \n",
      "\n",
      "\n",
      "Hugging Face response with LoRA:\n",
      "### Instruction: \n",
      "You are a knowledgeable history tutor. Answer the following question accurately based on the provided historical context.\n",
      "Use ONLY the information provided in the context. If you don't know, say \"I don't have enough information to answer that.\"\n",
      "\n",
      "### Context:\n",
      "Jiului and the strike in the Grivița railroad workshops. In the mid-1930s, the Romanian economy recovered and the industry grew significantly, although about 80% of Romanians were still employed in agriculture. French economic and political influence was predominant in the early 1920s but then Germany became more dominant, especially in the 1930s. As the 1930s progressed, Romania's already shaky democracy slowly deteriorated toward fascist dictatorship. The constitution of 1923 gave the king free rein to dissolve parliament and call elections at will; as a result, Romania was to experience over 25 governments in a single decade. Increasingly, these governments were dominated by a number of antisemitic, ultra-nationalist, and mostly at least quasi-fascist parties.\n",
      "\n",
      "In Romania proper, Soviet occupation following World War II facilitated the rise of the Communist Party as the main political force, leading ultimately to the forced abdication of the King and the establishment of a single-party people's republic in 1947. Romania was proclaimed a people's republic and remained under military and economic control of the Soviet Union until the late 1950s. During this period, Romania's resources were drained by the \"SovRom\" agreements; mixed Soviet-Romanian companies were established to mask the Soviet Union's looting of Romania. Romania's leader from 1948 to his death in 1965 was Gheorghe Gheorghiu-Dej, the First Secretary of the Romanian Workers' Party. The Communist regime was formalised with the constitution of 13 April 1948.\n",
      "\n",
      "against Romania, the capture of Bucharest and the signing of the Treaty of Bucharest. During the 20th century, Bulgaria and Romania both fought on the side of Nazi Germany during the Second World War. In the cold war, both countries became communist states under the influence of the Soviet Union, but Romania formally left the sphere in 1964. The communist regimes of both countries ultimately collapsed in 1989.\n",
      "\n",
      "During the first half of the 20th century, Romania and Bulgaria had a serious conflict over the Dobruja region. This dispute, while now largely forgotten, escalated into all out war in 1913. Romania participated in the Second Balkan War against Bulgaria. According to the Treaty of Bucharest concluded after the Bulgarian defeat, Bulgaria had to hand over Southern Dobruja region to Romania. The territorial dispute between the two countries ended with the Treaty of Craiova in 1940. After the treaty, the two countries carried out a population exchange in the affected areas.\n",
      "\n",
      "\n",
      "\n",
      "### Question:\n",
      "What happend in the 20th century in Romania?\n",
      "\n",
      "### Answer:\n",
      "Romania and Bulgaria had a serious conflict over the Dobruja region. This dispute, while now largely forgotten, escalated into all out war in 1913. Romania participated in the Second Balkan War against Bulgaria. According to the Treaty of Bucharest concluded after the Bulgarian defeat, Bulgaria had to hand over Southern Dobruja region to Romania. The territorial dispute between the two countries ended with the Treaty of Craiova in 1940. After the treaty, the two countries carried out a population exchange in the affected areas. \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 197
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T05:33:02.881533300Z",
     "start_time": "2025-05-13T04:09:05.874678Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6924a11b112e820d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
